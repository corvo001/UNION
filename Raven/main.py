import json
import os
import re
import shutil
import numpy as np
from datetime import datetime
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import cv2
import asyncio  # NUEVO: Necesario para funciones async

# Importaciones del sistema existente
from core.fractal_interpreter import FractalInterpreter
from core.pattern_classifier import PatternClassifier

# *** IMPORTACIÃ“N CORREGIDA DEL FOLDER ANALYZER ***
try:
    from core.folder_analyzer import FolderAnalyzer
    print("âœ… FolderAnalyzer importado correctamente")
except ImportError as e:
    print(f"âŒ Error importando FolderAnalyzer: {e}")
    print("ğŸ’¡ Verifica que core/folder_analyzer.py exista y estÃ© actualizado")
    exit(1)

# Nuevas importaciones para funcionalidades mejoradas
from core.hausdorff_extractor import HausdorffDimensionExtractor, ContourAnalysisExtractor
from core.knowledge_base import EnhancedKnowledgeBase

# *** IMPORTACIÃ“N DE RAVEN UNIVERSAL ***
try:
    from core.raven_universal import RavenUniversalAnalyzer, interactive_raven_universal
    UNIVERSAL_AVAILABLE = True
    print("âœ… Analizador Universal importado correctamente")
except ImportError as e:
    print(f"âš ï¸ Raven Universal no encontrado - funcionando en modo clÃ¡sico: {e}")
    UNIVERSAL_AVAILABLE = False
    # Crear funciÃ³n dummy para evitar errores
    def interactive_raven_universal():
        print("âŒ Analizador Universal no disponible")
        print("ğŸ’¡ Instala el mÃ³dulo raven_universal.py para habilitarlo")

# *** IMPORTACIONES PARA APRENDIZAJE GRATUITO ***
try:
    from core.free_learning import RavenFreeLearningSystem, interactive_free_learning
    FREE_LEARNING_AVAILABLE = True
    print("âœ… Sistema de aprendizaje gratuito cargado")
except ImportError as e:
    print(f"âš ï¸ Aprendizaje gratuito no disponible: {e}")
    FREE_LEARNING_AVAILABLE = False
    def interactive_free_learning():
        print("âŒ Sistema de aprendizaje gratuito no disponible")

# *** IMPORTACIONES PARA ENTRENAMIENTO AI ***
try:
    from core.training_mode import RavenTrainingMode, interactive_training_mode, TrainedRavenEnhancement
    TRAINING_MODE_AVAILABLE = True
    print("âœ… Modo de entrenamiento AI cargado")
except ImportError as e:
    print(f"âš ï¸ Modo de entrenamiento no disponible: {e}")
    TRAINING_MODE_AVAILABLE = False
    async def interactive_training_mode():
        print("âŒ Modo de entrenamiento AI no disponible")
    class TrainedRavenEnhancement:
        def __init__(self): self.has_trained_knowledge = False
        def enhance_classification(self, analysis, features): return analysis

# *** NUEVAS IMPORTACIONES PARA INTEGRACIÃ“N AI ***
try:
    from core.ai_integration import RavenAIIntegration, EnhancedRavenWithAI
    AI_INTEGRATION_AVAILABLE = True
    print("âœ… MÃ³dulo de integraciÃ³n AI cargado correctamente")
except ImportError as e:
    print(f"âš ï¸ IntegraciÃ³n AI no disponible: {e}")
    print("ğŸ’¡ Para habilitar AI: pip install openai anthropic")
    AI_INTEGRATION_AVAILABLE = False
    # Crear clases dummy para evitar errores
    class RavenAIIntegration:
        def __init__(self, *args, **kwargs):
            pass
    class EnhancedRavenWithAI:
        def __init__(self, *args, **kwargs):
            pass

# *** NUEVA FUNCIÃ“N PARA VERIFICAR AI EN TIEMPO REAL ***
def check_ai_integration_runtime():
    """
    VerificaciÃ³n mejorada en tiempo de ejecuciÃ³n para detectar si las dependencias de IA estÃ¡n disponibles.
    Esta funciÃ³n reemplaza la verificaciÃ³n estÃ¡tica y es mÃ¡s confiable.
    """
    ai_status = {
        'anthropic': False,
        'openai': False,
        'available': False
    }
    
    # Verificar Anthropic
    try:
        import anthropic
        # Test bÃ¡sico de funcionamiento
        try:
            # Solo test de importaciÃ³n sin crear cliente real
            ai_status['anthropic'] = True
            print("âœ… Anthropic SDK disponible")
        except Exception:
            ai_status['anthropic'] = True  # Si se puede importar, estÃ¡ disponible
            print("âœ… Anthropic SDK disponible")
    except ImportError:
        print("âŒ Anthropic SDK no encontrado")
    except Exception as e:
        print(f"âš ï¸ Anthropic SDK con problema: {e}")
    
    # Verificar OpenAI
    try:
        import openai
        ai_status['openai'] = True
        print("âœ… OpenAI SDK disponible")
    except ImportError:
        print("âŒ OpenAI SDK no encontrado")
    except Exception as e:
        print(f"âš ï¸ OpenAI SDK con problema: {e}")
    
    # Determinar disponibilidad general
    ai_status['available'] = ai_status['anthropic'] or ai_status['openai']
    
    return ai_status

def natural_sort_key(s):
    """FunciÃ³n para ordenar numÃ©ricamente los nombres de archivo."""
    return [int(text) if text.isdigit() else text.lower() 
            for text in re.split('([0-9]+)', s)]

def save_enhanced_classification(image_path, cluster, description, features, analysis):
    """Guarda clasificaciÃ³n mejorada con anÃ¡lisis detallado en JSON"""
    json_path = os.path.splitext(image_path)[0] + '.json'
    
    # Convertir arrays numpy a listas para JSON serialization
    serializable_features = {}
    for key, value in features.items():
        if isinstance(value, np.ndarray):
            serializable_features[key] = value.tolist()
        elif isinstance(value, (np.integer, np.floating)):
            serializable_features[key] = float(value)
        else:
            serializable_features[key] = value
    
    classification_data = {
        'image_filename': os.path.basename(image_path),
        'cluster': int(cluster),
        'cluster_name': analysis.get('cluster_name', f'Cluster {cluster}'),
        'cluster_description': description,
        'confidence': float(analysis.get('confidence', 0.0)),
        'fractal_features': serializable_features,
        'feature_analysis': analysis.get('feature_matches', {}),
        'similar_clusters': analysis.get('similar_clusters', []),
        'recommendations': analysis.get('recommendations', []),
        'classification_method': {
            'kmeans_prediction': analysis.get('kmeans_prediction', cluster),
            'rule_prediction': analysis.get('rule_prediction', cluster),
            'final_method': analysis.get('final_method', 'hybrid')
        },
        'last_processed': datetime.now().strftime("%d-%m-%Y %H:%M:%S"), 
        'version': '2.1_complete_system'
    }
    
    # *** AÃ‘ADIR MEJORAS DE ENTRENAMIENTO SI ESTÃN DISPONIBLES ***
    if analysis.get('used_trained_knowledge'):
        classification_data['trained_enhancements'] = {
            'improvements': analysis.get('trained_improvements', []),
            'confidence_boost': analysis.get('confidence_boost_from_training', 0.0),
            'enhanced_by_training': True
        }
    
    try:
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(classification_data, f, indent=4, ensure_ascii=False)
        print(f"ğŸ“Š AnÃ¡lisis guardado: {os.path.basename(json_path)}")
        if analysis.get('used_trained_knowledge'):
            print(f"ğŸ§  Incluye mejoras de entrenamiento AI")
    except Exception as e:
        print(f"âŒ Error guardando JSON para {image_path}: {e}")

# *** NUEVA FUNCIÃ“N PARA GUARDAR CON AI INSIGHTS ***
def save_enhanced_classification_with_ai(image_path, cluster, description, features, analysis, ai_insights=None):
    """Guarda clasificaciÃ³n mejorada con anÃ¡lisis detallado en JSON + insights AI"""
    json_path = os.path.splitext(image_path)[0] + '.json'
    
    # Convertir arrays numpy a listas para JSON serialization
    serializable_features = {}
    for key, value in features.items():
        if isinstance(value, np.ndarray):
            serializable_features[key] = value.tolist()
        elif isinstance(value, (np.integer, np.floating)):
            serializable_features[key] = float(value)
        else:
            serializable_features[key] = value
    
    classification_data = {
        'image_filename': os.path.basename(image_path),
        'cluster': int(cluster),
        'cluster_name': analysis.get('cluster_name', f'Cluster {cluster}'),
        'cluster_description': description,
        'confidence': float(analysis.get('confidence', 0.0)),
        'fractal_features': serializable_features,
        'feature_analysis': analysis.get('feature_matches', {}),
        'similar_clusters': analysis.get('similar_clusters', []),
        'recommendations': analysis.get('recommendations', []),
        'classification_method': {
            'kmeans_prediction': analysis.get('kmeans_prediction', cluster),
            'rule_prediction': analysis.get('rule_prediction', cluster),
            'final_method': analysis.get('final_method', 'hybrid')
        },
        'last_processed': datetime.now().strftime("%d-%m-%Y %H:%M:%S"), 
        'version': '2.1_ai_integrated'
    }
    
    # *** AÃ‘ADIR INSIGHTS AI SI ESTÃN DISPONIBLES ***
    if ai_insights:
        classification_data['ai_analysis'] = {
            'consensus_level': ai_insights.get('consensus', {}).get('agreement_level', 'none'),
            'ai_confidence': ai_insights.get('consensus', {}).get('confidence_avg', 0.0),
            'ai_recommendation': ai_insights.get('recommendation', ''),
            'models_consulted': [r['model'] for r in ai_insights.get('responses', [])],
            'confidence_boost': ai_insights.get('confidence_boost', 0.0)
        }
    
    try:
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(classification_data, f, indent=4, ensure_ascii=False)
        print(f"ğŸ“Š AnÃ¡lisis guardado: {os.path.basename(json_path)}")
        if ai_insights:
            print(f"ğŸ¤– Incluye insights AI: {ai_insights.get('consensus', {}).get('agreement_level', 'none')} consensus")
    except Exception as e:
        print(f"âŒ Error guardando JSON para {image_path}: {e}")

def extract_comprehensive_features(interpreter, hausdorff_extractor, contour_extractor, image_path):
    """
    Extrae caracterÃ­sticas completas usando todos los extractores.
    """
    try:
        # CaracterÃ­sticas bÃ¡sicas
        basic_features = interpreter.extract_features(image_path)
        
        # Cargar imagen para extractores adicionales
        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        if image is None:
            raise ValueError(f"No se pudo cargar la imagen: {image_path}")
        
        # CaracterÃ­sticas de dimensiÃ³n de Hausdorff
        hausdorff_features = hausdorff_extractor.extract(image)
        
        # CaracterÃ­sticas de contornos
        contour_features = contour_extractor.extract(image)
        
        # Combinar todas las caracterÃ­sticas
        comprehensive_features = {
            # CaracterÃ­sticas bÃ¡sicas
            'histogram': basic_features.histogram,
            'hu_moments': basic_features.hu_moments,
            'edge_density': basic_features.edge_density,
            'total_pixels': basic_features.total_pixels,
            
            # CaracterÃ­sticas de Hausdorff
            'hausdorff_dimension': hausdorff_features['hausdorff_dimension'],
            'local_dimensions': hausdorff_features['local_dimensions'],
            'dimension_variance': hausdorff_features['dimension_variance'],
            'dimension_complexity': hausdorff_features['dimension_complexity'],
            'fractal_type': hausdorff_features['fractal_type'],
            
            # CaracterÃ­sticas de contornos
            'contour_count': contour_features['contour_count'],
            'total_perimeter': contour_features['total_perimeter'],
            'avg_area': contour_features['avg_area'],
            'contour_complexity': contour_features['contour_complexity'],
            'circularity_mean': contour_features['circularity_mean'],
            'circularity_std': contour_features['circularity_std'],
            'convexity_mean': contour_features['convexity_mean'],
            'aspect_ratio_mean': contour_features['aspect_ratio_mean'],
            'contour_hierarchy_depth': contour_features['contour_hierarchy_depth']
        }
        
        return comprehensive_features
        
    except Exception as e:
        print(f"âŒ Error extrayendo caracterÃ­sticas de {image_path}: {e}")
        return None

def create_feature_vector(features):
    """Crea vector de caracterÃ­sticas para clustering."""
    try:
        vector_components = []
        
        # CaracterÃ­sticas bÃ¡sicas (histograma reducido)
        if 'histogram' in features and len(features['histogram']) > 0:
            hist_reduced = features['histogram'][:20]
            vector_components.extend(hist_reduced)
        else:
            vector_components.extend([0.0] * 20)
        
        # Momentos de Hu
        if 'hu_moments' in features and len(features['hu_moments']) > 0:
            vector_components.extend(features['hu_moments'])
        else:
            vector_components.extend([0.0] * 7)
        
        # CaracterÃ­sticas de Hausdorff
        vector_components.extend([
            features.get('hausdorff_dimension', 0.0),
            features.get('dimension_complexity', 0.0),
            features.get('dimension_variance', 0.0)
        ])
        
        # CaracterÃ­sticas de contornos mÃ¡s importantes
        vector_components.extend([
            features.get('edge_density', 0.0),
            features.get('circularity_mean', 0.0),
            features.get('contour_complexity', 0.0),
            features.get('convexity_mean', 0.0),
            np.log1p(features.get('contour_count', 0))
        ])
        
        # EstadÃ­sticas de dimensiones locales
        local_dims = features.get('local_dimensions', np.zeros(16))
        if len(local_dims) > 0:
            vector_components.extend([
                np.mean(local_dims),
                np.std(local_dims),
                np.max(local_dims),
                np.min(local_dims)
            ])
        else:
            vector_components.extend([0.0, 0.0, 0.0, 0.0])
        
        return np.array(vector_components, dtype=np.float32)
        
    except Exception as e:
        print(f"âŒ Error creando vector de caracterÃ­sticas: {e}")
        return np.zeros(39, dtype=np.float32)

class FixedEnhancedPatternClassifier:
    """Clasificador mejorado CORREGIDO que funciona correctamente."""
    
    def __init__(self, n_clusters=10):
        self.n_clusters = n_clusters
        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        self.knowledge_base = EnhancedKnowledgeBase()
        self.is_fitted = False
        
    def fit(self, feature_vectors, comprehensive_features_list):
        """Entrena el clasificador con vectores de caracterÃ­sticas."""
        try:
            self.kmeans.fit(feature_vectors)
            self.is_fitted = True
            print(f"ğŸ¤– Clasificador entrenado con {len(feature_vectors)} muestras")
            
            # Mostrar estadÃ­sticas de clusters
            self._print_cluster_statistics(feature_vectors, comprehensive_features_list)
            
        except Exception as e:
            print(f"âŒ Error entrenando clasificador: {e}")
    
    def predict_enhanced(self, feature_vector, comprehensive_features):
        """
        *** VERSIÃ“N CORREGIDA ***
        PredicciÃ³n mejorada que funciona correctamente.
        """
        if not self.is_fitted:
            return 0, 0.0, {'error': 'Classifier not fitted'}
        
        try:
            # PredicciÃ³n por K-means
            kmeans_cluster = self.kmeans.predict([feature_vector])[0]
            
            # *** CORRECIÃ“N PRINCIPAL ***
            # AnÃ¡lisis basado en reglas usando base de conocimiento
            rule_cluster, rule_confidence, all_scores = self.knowledge_base.classify_by_features(
                comprehensive_features
            )
            
            # Combinar ambos enfoques
            final_cluster, confidence, final_method = self._combine_predictions_fixed(
                kmeans_cluster, rule_cluster, rule_confidence, all_scores
            )
            
            # *** GENERAR ANÃLISIS DETALLADO ***
            analysis = self.knowledge_base.get_cluster_analysis(final_cluster, comprehensive_features)
            analysis['confidence'] = confidence
            analysis['kmeans_prediction'] = int(kmeans_cluster)
            analysis['rule_prediction'] = int(rule_cluster)
            analysis['final_method'] = final_method
            analysis['all_cluster_scores'] = {str(k): float(v) for k, v in all_scores.items()}
            analysis['similar_clusters'] = self.knowledge_base.suggest_similar_clusters(
                final_cluster, comprehensive_features
            )
            
            return final_cluster, confidence, analysis
            
        except Exception as e:
            print(f"âŒ Error en predicciÃ³n mejorada: {e}")
            return 0, 0.0, {'error': str(e)}
    
    def _combine_predictions_fixed(self, kmeans_cluster, rule_cluster, rule_confidence, all_scores):
        """
        *** VERSIÃ“N CORREGIDA ***
        Combina predicciones de manera mÃ¡s inteligente.
        """
        # Si la confianza de las reglas es muy alta, usar predicciÃ³n por reglas
        if rule_confidence > 0.8:
            return rule_cluster, rule_confidence, 'rule_based_high_confidence'
        
        # Si las predicciones coinciden, aumentar confianza
        if kmeans_cluster == rule_cluster:
            combined_confidence = min(0.95, rule_confidence + 0.3)
            return rule_cluster, combined_confidence, 'hybrid_agreement'
        
        # Si no coinciden, evaluar cuÃ¡l es mÃ¡s confiable basado en scores
        kmeans_confidence = 0.6  # Confianza base para K-means
        
        # Si la diferencia de scores es significativa, usar reglas
        sorted_scores = sorted(all_scores.values(), reverse=True)
        score_gap = sorted_scores[0] - sorted_scores[1] if len(sorted_scores) > 1 else 0
        
        if score_gap > 0.2:  # Gap significativo en scores de reglas
            return rule_cluster, rule_confidence + score_gap * 0.5, 'rule_based_significant_gap'
        
        # Si la confianza de reglas es mayor, usar reglas
        if rule_confidence > kmeans_confidence:
            return rule_cluster, rule_confidence, 'rule_based_higher_confidence'
        else:
            # Usar K-means pero con confianza reducida por el desacuerdo
            return kmeans_cluster, kmeans_confidence * 0.7, 'kmeans_with_disagreement'
    
    def _print_cluster_statistics(self, feature_vectors, comprehensive_features_list):
        """Imprime estadÃ­sticas de los clusters formados."""
        try:
            labels = self.kmeans.labels_
            
            print("\nğŸ“Š EstadÃ­sticas de Clustering:")
            for cluster_id in range(self.n_clusters):
                cluster_mask = labels == cluster_id
                cluster_count = np.sum(cluster_mask)
                percentage = (cluster_count / len(labels)) * 100
                
                cluster_name = self.knowledge_base.get_cluster_name(cluster_id)
                print(f"  Cluster {cluster_id} ({cluster_name}): {cluster_count} imÃ¡genes ({percentage:.1f}%)")
                
                if cluster_count > 0:
                    # EstadÃ­sticas de dimensiÃ³n de Hausdorff para este cluster
                    hausdorff_dims = [
                        features.get('hausdorff_dimension', 0.0) 
                        for i, features in enumerate(comprehensive_features_list) 
                        if cluster_mask[i]
                    ]
                    if hausdorff_dims:
                        avg_hausdorff = np.mean(hausdorff_dims)
                        print(f"    - DimensiÃ³n Hausdorff promedio: {avg_hausdorff:.3f}")
                        
        except Exception as e:
            print(f"âŒ Error calculando estadÃ­sticas: {e}")

def run_fractal_analysis():
    """FunciÃ³n que ejecuta el anÃ¡lisis fractal original."""
    print("ğŸš€ Iniciando Raven v2.1 SISTEMA COMPLETO...")

    # Inicializar componentes
    interpreter = FractalInterpreter()
    hausdorff_extractor = HausdorffDimensionExtractor()
    contour_extractor = ContourAnalysisExtractor()
    
    # AÃ±adir extractores al intÃ©rprete
    interpreter.add_extractor('hausdorff', hausdorff_extractor)
    interpreter.add_extractor('contours', contour_extractor)
    
    # *** USAR CLASIFICADOR CORREGIDO ***
    enhanced_classifier = FixedEnhancedPatternClassifier(n_clusters=10)
    scaler = StandardScaler()

    # *** BUSCAR CARPETA DEL DÃA CON FOLDER ANALYZER CORREGIDO ***
    print("ğŸ” Buscando carpeta del dÃ­a con FolderAnalyzer corregido...")
    
    # Usar None para autodetecciÃ³n de rutas
    today_folder = FolderAnalyzer.get_todays_folder()

    if not today_folder:
        print("âš ï¸ No se encontrÃ³ carpeta del dÃ­a. Intentando crear...")
        today_folder = FolderAnalyzer.create_todays_folder()
        
        if not today_folder:
            print("âŒ No se pudo crear carpeta del dÃ­a.")
            print("ğŸ’¡ Verifica la estructura de carpetas o crea manualmente:")
            print("    data/today/DDMMYYYY")
            return
        else:
            print(f"âœ… Carpeta creada: {today_folder}")

    print(f"âœ… Carpeta de hoy encontrada: {today_folder}")

    # Verificar que la carpeta tenga contenido
    try:
        all_files = os.listdir(today_folder)
        print(f"ğŸ“ Total de archivos en carpeta: {len(all_files)}")
        
        if len(all_files) == 0:
            print("âš ï¸ La carpeta estÃ¡ vacÃ­a")
            print("ğŸ’¡ Copia imÃ¡genes fractales a la carpeta antes de ejecutar")
            return
        
        # Mostrar primeros archivos para debug
        print("ğŸ“‚ Primeros archivos encontrados:")
        for i, file in enumerate(all_files[:5]):
            print(f"   {i+1}. {file}")
        if len(all_files) > 5:
            print(f"   ... y {len(all_files) - 5} archivos mÃ¡s")
        
    except Exception as e:
        print(f"âŒ Error accediendo a la carpeta: {e}")
        return

    # *** FILTRO CORREGIDO ***
    # Obtener y ordenar archivos (VERSIÃ“N CORREGIDA)
    image_files = sorted(
        [f for f in os.listdir(today_folder) if f.lower().endswith((".png", ".jpg", ".jpeg"))],
        key=natural_sort_key
    )
    
    if not image_files:
        print("âŒ No se encontraron imÃ¡genes en la carpeta")
        print("ğŸ’¡ Formatos soportados: .png, .jpg, .jpeg")
        return

    print(f"ğŸ–¼ï¸ Encontradas {len(image_files)} imÃ¡genes para procesar")

    # Procesar imÃ¡genes y extraer caracterÃ­sticas completas
    feature_vectors = []
    comprehensive_features_list = []
    processed_files = []

    print("\nğŸ” Extrayendo caracterÃ­sticas avanzadas...")
    for i, fname in enumerate(image_files):
        path = os.path.join(today_folder, fname)
        print(f"  Procesando {i+1}/{len(image_files)}: {fname}")
        
        try:
            # Extraer caracterÃ­sticas completas
            comprehensive_features = extract_comprehensive_features(
                interpreter, hausdorff_extractor, contour_extractor, path
            )
            
            if comprehensive_features is not None:
                # Crear vector para clustering
                feature_vector = create_feature_vector(comprehensive_features)
                
                feature_vectors.append(feature_vector)
                comprehensive_features_list.append(comprehensive_features)
                processed_files.append(fname)
                
                # Mostrar dimensiÃ³n de Hausdorff y tipo fractal
                hausdorff_dim = comprehensive_features.get('hausdorff_dimension', 0.0)
                fractal_type = comprehensive_features.get('fractal_type', 'unknown')
                print(f"     DimensiÃ³n Hausdorff: {hausdorff_dim:.3f} | Tipo: {fractal_type}")
                
        except Exception as e:
            print(f"     âŒ Error procesando {fname}: {e}")

    if not feature_vectors:
        print("âŒ No se pudieron procesar imÃ¡genes vÃ¡lidas")
        return

    print(f"\nâœ… Procesadas {len(feature_vectors)} imÃ¡genes exitosamente")

    # Escalado y entrenamiento del clasificador
    print("\nğŸ¤– Entrenando clasificador hÃ­brido corregido...")
    feature_vectors_np = np.array(feature_vectors)
    scaled_vectors = scaler.fit_transform(feature_vectors_np)
    enhanced_classifier.fit(scaled_vectors, comprehensive_features_list)

    # ClasificaciÃ³n y anÃ¡lisis detallado
    print("\nğŸ“Š Resultados de clasificaciÃ³n hÃ­brida corregida:")
    print("=" * 80)
    
    classification_summary = {i: 0 for i in range(10)}
    
    for i, (feature_vector, comprehensive_features, fname) in enumerate(
        zip(feature_vectors_np, comprehensive_features_list, processed_files)
    ):
        img_path = os.path.join(today_folder, fname)
        scaled_vector = scaler.transform([feature_vector])[0]
        
        # *** PREDICCIÃ“N MEJORADA CORREGIDA CON ENTRENAMIENTO AI ***
        cluster, confidence, analysis = enhanced_classifier.predict_enhanced(
            scaled_vector, comprehensive_features
        )
        
        # Aplicar mejoras de conocimiento entrenado si estÃ¡ disponible
        if TRAINING_MODE_AVAILABLE:
            enhancement = TrainedRavenEnhancement()
            if enhancement.has_trained_knowledge:
                analysis = enhancement.enhance_classification(analysis, comprehensive_features)
                confidence = analysis.get('confidence', confidence)
        
        classification_summary[cluster] += 1
        
        # DescripciÃ³n del cluster
        cluster_name = analysis.get('cluster_name', f'Cluster {cluster}')
        description = enhanced_classifier.knowledge_base.describe_cluster(cluster)
        
        # Mostrar resultado detallado
        print(f"\nğŸ–¼ï¸ {fname}")
        print(f"     Cluster: {cluster} - {cluster_name}")
        print(f"    ğŸ“ {description}")
        print(f"    ğŸ¯ Confianza: {confidence:.3f} | MÃ©todo: {analysis.get('final_method', 'hybrid')}")
        
        # Mostrar si hay mejoras de entrenamiento
        if analysis.get('used_trained_knowledge'):
            boost = analysis.get('confidence_boost_from_training', 0.0)
            print(f"    ğŸ§  Mejorado con entrenamiento AI (+{boost:.3f} confianza)")
        
        # Mostrar caracterÃ­sticas clave
        hausdorff_dim = comprehensive_features.get('hausdorff_dimension', 0.0)
        complexity = comprehensive_features.get('dimension_complexity', 0.0)
        contour_count = comprehensive_features.get('contour_count', 0)
        circularity = comprehensive_features.get('circularity_mean', 0.0)
        
        print(f"    ğŸ“ Dim. Hausdorff: {hausdorff_dim:.3f} | Complejidad: {complexity:.3f}")
        print(f"    ğŸ” Contornos: {contour_count} | Circularidad: {circularity:.3f}")
        
        # Mostrar predicciones de ambos mÃ©todos
        kmeans_pred = analysis.get('kmeans_prediction', cluster)
        rule_pred = analysis.get('rule_prediction', cluster)
        print(f"    ğŸ¤– K-means: {kmeans_pred} | Reglas: {rule_pred}")
        
        # Mostrar clusters similares
        similar_clusters = analysis.get('similar_clusters', [])
        if similar_clusters and len(similar_clusters) > 0:
            similar_info = similar_clusters[0]
            similar_name = enhanced_classifier.knowledge_base.get_cluster_name(similar_info[0])
            print(f"    ğŸ”— Similar a: Cluster {similar_info[0]} ({similar_name}) - Score: {similar_info[1]:.3f}")
        
        # Mostrar recomendaciones
        recommendations = analysis.get('recommendations', [])
        if recommendations:
            print(f"    ğŸ’¡ RecomendaciÃ³n: {recommendations[0]}")
        
        # Guardar anÃ¡lisis completo
        save_enhanced_classification(img_path, cluster, description, 
                                   comprehensive_features, analysis)

    # Resumen final
    print("\n" + "=" * 80)
    print("ğŸ“‹ RESUMEN DE CLASIFICACIÃ“N COMPLETA")
    print("=" * 80)
    
    knowledge_base = enhanced_classifier.knowledge_base
    
    for cluster_id in range(10):
        count = classification_summary[cluster_id]
        if count > 0:
            percentage = (count / len(processed_files)) * 100
            cluster_name = knowledge_base.get_cluster_name(cluster_id)
            print(f"  Cluster {cluster_id} ({cluster_name}): {count} imÃ¡genes ({percentage:.1f}%)")
    
    # EstadÃ­sticas globales de dimensiÃ³n de Hausdorff
    all_hausdorff_dims = [f.get('hausdorff_dimension', 0.0) for f in comprehensive_features_list]
    if all_hausdorff_dims:
        avg_hausdorff = np.mean(all_hausdorff_dims)
        std_hausdorff = np.std(all_hausdorff_dims)
        min_hausdorff = np.min(all_hausdorff_dims)
        max_hausdorff = np.max(all_hausdorff_dims)
        
        print(f"\nğŸ“Š ESTADÃSTICAS DE DIMENSIÃ“N DE HAUSDORFF:")
        print(f"   Promedio: {avg_hausdorff:.3f} Â± {std_hausdorff:.3f}")
        print(f"   Rango: [{min_hausdorff:.3f}, {max_hausdorff:.3f}]")
    
    # DistribuciÃ³n de tipos fractales
    fractal_types = [f.get('fractal_type', 'unknown') for f in comprehensive_features_list]
    type_counts = {}
    for ftype in fractal_types:
        type_counts[ftype] = type_counts.get(ftype, 0) + 1
    
    print(f"\nğŸ”¬ DISTRIBUCIÃ“N DE TIPOS FRACTALES:")
    for ftype, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / len(fractal_types)) * 100
        print(f"   {ftype}: {count} imÃ¡genes ({percentage:.1f}%)")

    # *** MOVER CARPETA PROCESADA CON FOLDER ANALYZER ***
    print(f"\nğŸ“¦ Moviendo carpeta procesada...")
    
    # Obtener ruta base del proyecto
    project_root = FolderAnalyzer._get_project_root()
    processed_dir = os.path.join(project_root, "data", "processed")
    
    if not os.path.exists(processed_dir):
        os.makedirs(processed_dir)
    
    # Obtener nombre base de la carpeta
    folder_name = datetime.now().strftime("%d%m%Y")
    
    # Buscar un nombre Ãºnico con numeraciÃ³n automÃ¡tica
    counter = 1
    while True:
        new_folder_name = f"{folder_name}_analyzed_{counter}"
        destination = os.path.join(processed_dir, new_folder_name)
        
        # Si no existe esta carpeta, usar este nombre
        if not os.path.exists(destination):
            break
        
        # Si existe, incrementar contador y probar siguiente
        counter += 1
        
        # Prevenir bucle infinito (mÃ¡ximo 999 intentos)
        if counter > 999:
            print(f"âš ï¸ Demasiadas carpetas con la misma fecha. Usando timestamp.")
            timestamp = datetime.now().strftime("%d%m%Y_%H%M%S")
            new_folder_name = f"{folder_name}_analyzed_{timestamp}"
            destination = os.path.join(processed_dir, new_folder_name)
            break
    
    try:
        shutil.move(today_folder, destination)
        print(f"âœ… Carpeta movida a procesados: {destination}")
        print(f"ğŸ“ Nombre asignado: {folder_name} â†’ {new_folder_name}")
        if counter > 1:
            print(f"ğŸ“Š AnÃ¡lisis #{counter} del dÃ­a")
    except Exception as e:
        print(f"âŒ Error moviendo carpeta: {e}")

    print(f"\nğŸ‰ Â¡ClasificaciÃ³n hÃ­brida completada!")

# *** NUEVA FUNCIÃ“N PARA ANÃLISIS FRACTAL CON AI ***
async def run_fractal_analysis_with_ai(openai_key=None, anthropic_key=None):
    """FunciÃ³n que ejecuta el anÃ¡lisis fractal con integraciÃ³n AI."""
    print("ğŸš€ Iniciando Raven v2.1 con INTEGRACIÃ“N AI...")
    
    # Verificar disponibilidad de AI
    if not AI_INTEGRATION_AVAILABLE:
        print("âš ï¸ IntegraciÃ³n AI no disponible, ejecutando anÃ¡lisis clÃ¡sico")
        run_fractal_analysis()  # Llamar funciÃ³n original
        return
    
    if not (openai_key or anthropic_key):
        print("âš ï¸ No se proporcionaron claves API, ejecutando anÃ¡lisis clÃ¡sico")
        run_fractal_analysis()  # Llamar funciÃ³n original
        return
    
    # Crear instancia mejorada de Raven con AI
    try:
        enhanced_raven = EnhancedRavenWithAI(openai_key, anthropic_key)
        print("ğŸ¤– Raven con capacidades AI iniciado exitosamente")
    except Exception as e:
        print(f"âŒ Error iniciando integraciÃ³n AI: {e}")
        print("ğŸ”„ Continuando con anÃ¡lisis clÃ¡sico...")
        run_fractal_analysis()
        return
    
    print("ğŸ” Buscando carpeta del dÃ­a...")
    today_folder = FolderAnalyzer.get_todays_folder()
    
    if not today_folder:
        print("âŒ No se encontrÃ³ carpeta del dÃ­a")
        return
    
    # Obtener imÃ¡genes
    image_files = sorted(
        [f for f in os.listdir(today_folder) if f.lower().endswith((".png", ".jpg", ".jpeg"))],
        key=natural_sort_key
    )
    
    if not image_files:
        print("âŒ No se encontraron imÃ¡genes")
        return
    
    print(f"ğŸ–¼ï¸ Encontradas {len(image_files)} imÃ¡genes para anÃ¡lisis AI-mejorado")
    
    # Procesar cada imagen con anÃ¡lisis AI (ejemplo para primeras 3 imÃ¡genes)
    for i, fname in enumerate(image_files[:3]):  # Procesar solo primeras 3 para demo
        img_path = os.path.join(today_folder, fname)
        print(f"\nğŸ” Analizando con AI: {fname} ({i+1}/{min(3, len(image_files))})")
        
        try:
            # AnÃ¡lisis mejorado con consenso AI
            enhanced_analysis = await enhanced_raven.analyze_with_ai_consensus(img_path)
            
            # Mostrar resultados
            raven_cluster = enhanced_analysis['raven_analysis']['cluster_analysis']['cluster_name']
            ai_agreement = enhanced_analysis['ai_consensus']['consensus']['agreement_level']
            final_confidence = enhanced_analysis['confidence_boost']
            
            print(f"ğŸ“Š AnÃ¡lisis Raven: {raven_cluster}")
            print(f"ğŸ¤– Consenso AI: {ai_agreement}")
            print(f"âœ¨ Confianza final: {final_confidence:.3f}")
            
            # Guardar con insights AI
            save_enhanced_classification_with_ai(
                img_path,
                enhanced_analysis['raven_analysis']['cluster_analysis'].get('cluster_id', 0),
                enhanced_analysis['raven_analysis']['cluster_analysis'].get('cluster_name', ''),
                enhanced_analysis['raven_analysis']['features'],
                enhanced_analysis['raven_analysis']['cluster_analysis'],
                enhanced_analysis['ai_consensus']
            )
            
        except Exception as e:
            print(f"âŒ Error en anÃ¡lisis AI para {fname}: {e}")
            continue
    
    print("\nğŸ‰ Â¡AnÃ¡lisis fractal con AI completado!")

# *** FUNCIÃ“N SHOW_MAIN_MENU CORREGIDA CON DETECCIÃ“N AI EN TIEMPO REAL ***
def show_main_menu():
    """Muestra el menÃº principal integrado con todas las opciones."""
    print("\n" + "=" * 70)
    print("ğŸ¦… RAVEN - SISTEMA DE ANÃLISIS FRACTAL INTEGRADO v2.1 COMPLETO")
    print("=" * 70)
    print("\nğŸ¯ Selecciona el modo de anÃ¡lisis:")
    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    
    print("\n1. ğŸ“Š AnÃ¡lisis Fractal de Carpeta (Modo ClÃ¡sico)")
    print("   â””â”€ Procesamiento tradicional de Raven")
    print("   â””â”€ ClasificaciÃ³n en 10 clusters especializados")
    print("   â””â”€ AnÃ¡lisis de dimensiÃ³n de Hausdorff avanzado")
    print("   â””â”€ 100% GRATUITO")
    
    if FREE_LEARNING_AVAILABLE:
        print("\n2. ğŸ§  Aprendizaje Gratuito")
        print("   â””â”€ Raven aprende de tus anÃ¡lisis histÃ³ricos")
        print("   â””â”€ Correcciones manuales para mejorar precisiÃ³n")
        print("   â””â”€ Auto-mejora basada en tus datos")
        print("   â””â”€ 100% GRATUITO - Sin APIs externas")
    else:
        print("\n2. ğŸ§  Aprendizaje Gratuito (No disponible)")
        print("   â””â”€ Falta core/free_learning.py")
    
    if TRAINING_MODE_AVAILABLE:
        print("\n3. ğŸ“ Entrenar Raven con AI (Una vez)")
        print("   â””â”€ GPT-4 + Claude entrenan a Raven")
        print("   â””â”€ InversiÃ³n Ãºnica: $5-15")
        print("   â””â”€ DespuÃ©s funciona GRATIS para siempre")
        print("   â””â”€ Mejora permanente del sistema")
    else:
        print("\n3. ğŸ“ Entrenar Raven con AI (No disponible)")
        print("   â””â”€ Falta core/training_mode.py o dependencias")
    
    # *** VERIFICACIÃ“N AI MEJORADA EN TIEMPO REAL ***
    print("\nğŸ” Verificando dependencias AI...")
    ai_runtime_status = check_ai_integration_runtime()
    
    if ai_runtime_status['available']:
        print("\n4. ğŸ¤– AnÃ¡lisis Fractal con AI (GPT-4 + Claude)")
        print("   â””â”€ AnÃ¡lisis Raven + consenso de modelos AI")
        print("   â””â”€ ClasificaciÃ³n verificada por GPT-4 y Claude")
        print("   â””â”€ Confianza mejorada basada en consenso")
        print("   â””â”€ âš ï¸ Requiere claves API vÃ¡lidas")
        
        # Mostrar quÃ© SDKs estÃ¡n disponibles
        available_models = []
        if ai_runtime_status['anthropic']:
            available_models.append("Claude")
        if ai_runtime_status['openai']:
            available_models.append("GPT-4")
        print(f"   â””â”€ SDKs disponibles: {', '.join(available_models)}")
    else:
        print("\n4. âŒ AnÃ¡lisis Fractal con AI (No disponible)")
        print("   â””â”€ Dependencias faltantes detectadas:")
        if not ai_runtime_status['anthropic']:
            print("       â€¢ pip install anthropic")
        if not ai_runtime_status['openai']:
            print("       â€¢ pip install openai")
    
    if UNIVERSAL_AVAILABLE:
        print("\n5. ğŸŒŸ AnÃ¡lisis Universal")
        print("   â””â”€ Analiza CUALQUIER tipo de datos")
        print("   â””â”€ Texto, nÃºmeros, URLs, archivos, JSON, CSV")
    else:
        print("\n5. ğŸŒŸ AnÃ¡lisis Universal (No disponible)")
        print("   â””â”€ Falta core/raven_universal.py")
    
    print("\n6. ğŸ“‹ InformaciÃ³n del Sistema")
    print("7. ğŸšª Salir")
    
    # *** MOSTRAR ESTADOS DE DISPONIBILIDAD ACTUALIZADOS ***
    print(f"\nğŸ“Š Estado de funcionalidades (verificado en tiempo real):")
    print(f"   ğŸ”„ AnÃ¡lisis ClÃ¡sico: âœ… Siempre disponible")
    print(f"   ğŸ§  Aprendizaje Gratuito: {'âœ…' if FREE_LEARNING_AVAILABLE else 'âŒ'}")
    print(f"   ğŸ“ Entrenamiento AI: {'âœ…' if TRAINING_MODE_AVAILABLE else 'âŒ'}")
    print(f"   ğŸ¤– IntegraciÃ³n AI: {'âœ…' if ai_runtime_status['available'] else 'âŒ'}")
    print(f"   ğŸŒŸ AnÃ¡lisis Universal: {'âœ…' if UNIVERSAL_AVAILABLE else 'âŒ'}")
    
    # Guardar estado AI para uso en main()
    global AI_RUNTIME_AVAILABLE
    AI_RUNTIME_AVAILABLE = ai_runtime_status['available']

def show_system_info():
    """Muestra informaciÃ³n completa del sistema."""
    print("\n" + "=" * 60)
    print("ğŸ“‹ INFORMACIÃ“N DEL SISTEMA RAVEN v2.1 COMPLETO")
    print("=" * 60)
    
    print("\nğŸ”§ COMPONENTES PRINCIPALES:")
    components = [
        ("IntÃ©rprete Fractal", True),
        ("Clasificador de Patrones", True), 
        ("Analizador de Carpetas", True),
        ("Extractor Hausdorff", True),
        ("Base de Conocimiento (10 clusters)", True),
        ("AnÃ¡lisis Universal", UNIVERSAL_AVAILABLE),
        ("Aprendizaje Gratuito", FREE_LEARNING_AVAILABLE),
        ("Entrenamiento AI", TRAINING_MODE_AVAILABLE),
        ("IntegraciÃ³n AI", AI_INTEGRATION_AVAILABLE)
    ]
    
    for name, available in components:
        status = "âœ… Disponible" if available else "âŒ No disponible"
        print(f"   {name}: {status}")
    
    print(f"\nğŸ¯ CAPACIDADES ACTIVAS:")
    print(f"   â€¢ AnÃ¡lisis fractal especializado: âœ…")
    print(f"   â€¢ ClasificaciÃ³n en 10 clusters: âœ…")
    print(f"   â€¢ DimensiÃ³n de Hausdorff: âœ…")
    print(f"   â€¢ AnÃ¡lisis de contornos: âœ…")
    print(f"   â€¢ Base de conocimiento expandida: âœ…")
    print(f"   â€¢ Aprendizaje de datos histÃ³ricos: {'âœ…' if FREE_LEARNING_AVAILABLE else 'âŒ'}")
    print(f"   â€¢ Entrenamiento con AI: {'âœ…' if TRAINING_MODE_AVAILABLE else 'âŒ'}")
    print(f"   â€¢ IntegraciÃ³n AI en tiempo real: {'âœ…' if AI_INTEGRATION_AVAILABLE else 'âŒ'}")
    print(f"   â€¢ AnÃ¡lisis universal: {'âœ…' if UNIVERSAL_AVAILABLE else 'âŒ'}")
    
    # Verificar si hay conocimiento entrenado
    if TRAINING_MODE_AVAILABLE:
        enhancement = TrainedRavenEnhancement()
        if enhancement.has_trained_knowledge:
            print(f"   ğŸ§  Conocimiento AI entrenado: âœ… ACTIVO")
            print(f"       â””â”€ Raven funcionarÃ¡ con mejoras AI automÃ¡ticamente")
        else:
            print(f"   ğŸ§  Conocimiento AI entrenado: âŒ Sin entrenar")
            print(f"       â””â”€ Usa opciÃ³n 3 para entrenar Raven con AI")
    
    print(f"\nğŸ“Š TIPOS DE FRACTALES RECONOCIDOS:")
    try:
        kb = EnhancedKnowledgeBase()
        for i in range(10):
            cluster_name = kb.get_cluster_name(i)
            print(f"   {i}. {cluster_name}")
    except Exception as e:
        print(f"   âŒ Error accediendo a la base de conocimiento: {e}")
    
    print(f"\nğŸ—‚ï¸  ESTRUCTURA DE DATOS:")
    print(f"   ğŸ“ data/ - Carpetas de imÃ¡genes por fecha")
    print(f"   ğŸ“ data/processed/ - AnÃ¡lisis completados")
    print(f"   ğŸ“ data/trained_knowledge/ - Conocimiento AI entrenado")
    print(f"   ğŸ“„ *.json - Metadatos de cada imagen")

    # Mostrar estado actual de carpetas
    print(f"\nğŸ“‹ ESTADO ACTUAL:")
    try:
        # Verificar carpeta del dÃ­a
        today_folder = FolderAnalyzer.get_todays_folder()
        if today_folder:
            print(f"   âœ… Carpeta del dÃ­a: {os.path.basename(today_folder)}")
            stats = FolderAnalyzer.get_folder_stats(today_folder)
            if "error" not in stats:
                print(f"   ğŸ“Š Archivos disponibles: {stats['files']}")
                if stats['file_types']:
                    for ext, count in stats['file_types'].items():
                        print(f"      {ext}: {count}")
        else:
            print(f"   âŒ No hay carpeta del dÃ­a actual")
        
        # Fechas disponibles
        dates = FolderAnalyzer.list_available_dates()
        if dates:
            print(f"   ğŸ“… Fechas disponibles: {len(dates)}")
            for date in dates[:3]:
                formatted = f"{date[0:2]}/{date[2:4]}/{date[4:8]}"
                print(f"      â€¢ {date} ({formatted})")
            if len(dates) > 3:
                print(f"      ... y {len(dates) - 3} mÃ¡s")
        else:
            print(f"   ğŸ“… No hay fechas disponibles")
            
    except Exception as e:
        print(f"   âš ï¸ Error verificando estado: {e}")

# *** FUNCIÃ“N MAIN CORREGIDA ***
def main():
    """FunciÃ³n principal con menÃº integrado completo."""
    
    print("ğŸš€ INICIANDO RAVEN v2.1 - SISTEMA COMPLETO")
    print("=" * 60)
    
    # Verificar FolderAnalyzer al inicio
    try:
        test_result = FolderAnalyzer._get_project_root()
        print(f"âœ… FolderAnalyzer funcionando - Proyecto en: {test_result}")
    except Exception as e:
        print(f"âŒ Error en FolderAnalyzer: {e}")
        print("ğŸ’¡ Verifica que core/folder_analyzer.py estÃ© actualizado")
        input("Presiona Enter para continuar de todos modos...")
    
    # *** VERIFICACIÃ“N INICIAL DE DEPENDENCIAS AI ***
    print("\nğŸ” VerificaciÃ³n inicial de dependencias AI...")
    ai_initial_check = check_ai_integration_runtime()
    if ai_initial_check['available']:
        print("âœ… Dependencias AI detectadas correctamente")
    else:
        print("âš ï¸ Algunas dependencias AI no estÃ¡n disponibles")
    
    while True:
        show_main_menu()
        
        try:
            choice = input("\nğŸ‘‰ Selecciona una opciÃ³n (1-7): ").strip()
            
            if choice == '1':
                print("\nğŸ”„ Iniciando anÃ¡lisis fractal clÃ¡sico...")
                run_fractal_analysis()
                input("\nâœ¨ Presiona Enter para continuar...")
                
            elif choice == '2':
                if FREE_LEARNING_AVAILABLE:
                    print("\nğŸ§  Iniciando sistema de aprendizaje gratuito...")
                    try:
                        interactive_free_learning()
                    except Exception as e:
                        print(f"âŒ Error en aprendizaje gratuito: {e}")
                    input("\nâœ¨ Presiona Enter para continuar...")
                else:
                    print("\nâŒ Aprendizaje gratuito no disponible")
                    print("ğŸ’¡ Crea el archivo core/free_learning.py")
                    input("\nâœ¨ Presiona Enter para continuar...")
                    
            elif choice == '3':
                if TRAINING_MODE_AVAILABLE:
                    print("\nğŸ“ Iniciando modo de entrenamiento AI...")
                    try:
                        # Verificar si ya estÃ¡ entrenado
                        enhancement = TrainedRavenEnhancement()
                        if enhancement.has_trained_knowledge:
                            print("ğŸ§  Raven ya tiene conocimiento entrenado!")
                            print("âœ… El sistema ya funciona con mejoras AI")
                            
                            retrain = input("Â¿Quieres volver a entrenar? (s/n): ").strip().lower()
                            if retrain not in ['s', 'sÃ­', 'si', 'y', 'yes']:
                                continue
                        
                        # Ejecutar entrenamiento
                        asyncio.run(interactive_training_mode())
                    except Exception as e:
                        print(f"âŒ Error en entrenamiento: {e}")
                    input("\nâœ¨ Presiona Enter para continuar...")
                else:
                    print("\nâŒ Modo de entrenamiento no disponible")
                    print("ğŸ’¡ Crea core/training_mode.py e instala: pip install openai anthropic")
                    input("\nâœ¨ Presiona Enter para continuar...")
                    
            elif choice == '4':
                # *** VERIFICACIÃ“N AI CORREGIDA ***
                print("\nğŸ” Verificando disponibilidad AI en tiempo real...")
                ai_check = check_ai_integration_runtime()
                
                if ai_check['available']:
                    print("âœ… AI disponible - Continuando con configuraciÃ³n...")
                    print("\nğŸ¤– CONFIGURACIÃ“N DE INTEGRACIÃ“N AI")
                    print("=" * 40)
                    print("Necesitas claves API para:")
                    if ai_check['openai']:
                        print("â€¢ âœ… OpenAI GPT-4: https://platform.openai.com/api-keys")
                    else:
                        print("â€¢ âŒ OpenAI no disponible")
                    if ai_check['anthropic']:
                        print("â€¢ âœ… Anthropic Claude: https://console.anthropic.com/")
                    else:
                        print("â€¢ âŒ Anthropic no disponible")
                    
                    print("\nâš ï¸ Las claves se usan solo para esta sesiÃ³n (no se almacenan)")
                    
                    # Solicitar claves API solo para los SDKs disponibles
                    openai_key = ""
                    anthropic_key = ""
                    
                    if ai_check['openai']:
                        openai_key = input("\nğŸ”‘ Clave OpenAI (Enter para omitir): ").strip()
                    
                    if ai_check['anthropic']:
                        anthropic_key = input("ğŸ”‘ Clave Anthropic (Enter para omitir): ").strip()
                    
                    if openai_key or anthropic_key:
                        print(f"\nğŸš€ Iniciando anÃ¡lisis AI...")
                        print(f"   GPT-4: {'âœ…' if openai_key and ai_check['openai'] else 'âŒ'}")
                        print(f"   Claude: {'âœ…' if anthropic_key and ai_check['anthropic'] else 'âŒ'}")
                        
                        # Ejecutar anÃ¡lisis AI de forma asÃ­ncrona
                        try:
                            if AI_INTEGRATION_AVAILABLE:  # Verificar que el mÃ³dulo estÃ¡ importado
                                asyncio.run(run_fractal_analysis_with_ai(openai_key, anthropic_key))
                            else:
                                print("âš ï¸ MÃ³dulo AI no cargado, ejecutando anÃ¡lisis clÃ¡sico")
                                run_fractal_analysis()
                        except Exception as e:
                            print(f"âŒ Error en anÃ¡lisis AI: {e}")
                            print("ğŸ”„ Ejecutando anÃ¡lisis clÃ¡sico...")
                            run_fractal_analysis()
                    else:
                        print("âŒ No se proporcionaron claves, ejecutando anÃ¡lisis clÃ¡sico")
                        run_fractal_analysis()
                    
                    input("\nâœ¨ Presiona Enter para continuar...")
                else:
                    print("\nâŒ IntegraciÃ³n AI no disponible")
                    print("ğŸ”§ Dependencias faltantes:")
                    if not ai_check['anthropic']:
                        print("   pip install anthropic")
                    if not ai_check['openai']:
                        print("   pip install openai")
                    print("\nğŸ’¡ Instala las dependencias y reinicia RAVEN")
                    input("\nâœ¨ Presiona Enter para continuar...")
                    
            elif choice == '5':
                if UNIVERSAL_AVAILABLE:
                    print("\nğŸŒŸ Iniciando Raven Universal...")
                    try:
                        interactive_raven_universal()
                    except Exception as e:
                        print(f"âŒ Error en anÃ¡lisis universal: {e}")
                    input("\nâœ¨ Presiona Enter para continuar...")
                else:
                    print("\nâŒ AnÃ¡lisis Universal no disponible")
                    print("ğŸ’¡ Instala core/raven_universal.py para habilitarlo")
                    input("\nâœ¨ Presiona Enter para continuar...")
                    
            elif choice == '6':
                show_system_info()
                input("\nâœ¨ Presiona Enter para continuar...")
                
            elif choice == '7':
                print("\nğŸ‘‹ Â¡Gracias por usar Raven v2.1 Completo!")
                print("ğŸ¦… Sistema de anÃ¡lisis fractal con mÃ¡ximas capacidades")
                print("ğŸ”§ Aprendizaje gratuito + Entrenamiento AI disponibles")
                break
                
            else:
                print("âŒ OpciÃ³n no vÃ¡lida. Selecciona 1-7.")
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ Salida interrumpida por el usuario")
            break
        except Exception as e:
            print(f"\nâŒ Error inesperado: {e}")
            import traceback
            traceback.print_exc()
            input("âœ¨ Presiona Enter para continuar...")

# *** INICIALIZAR VARIABLE GLOBAL ***
AI_RUNTIME_AVAILABLE = False

if __name__ == "__main__":
    main()