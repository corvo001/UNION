import json
import os
import re
import shutil
import numpy as np
from datetime import datetime
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import cv2
import asyncio  # NUEVO: Necesario para funciones async

# Importaciones del sistema existente
from core.fractal_interpreter import FractalInterpreter
from core.pattern_classifier import PatternClassifier

# *** IMPORTACI√ìN CORREGIDA DEL FOLDER ANALYZER ***
try:
    from core.folder_analyzer import FolderAnalyzer
    print("‚úÖ FolderAnalyzer importado correctamente")
except ImportError as e:
    print(f"‚ùå Error importando FolderAnalyzer: {e}")
    print("üí° Verifica que core/folder_analyzer.py exista y est√© actualizado")
    exit(1)

# Nuevas importaciones para funcionalidades mejoradas
from core.hausdorff_extractor import HausdorffDimensionExtractor, ContourAnalysisExtractor
from core.knowledge_base import EnhancedKnowledgeBase

# *** IMPORTACI√ìN DE RAVEN UNIVERSAL ***
try:
    from core.raven_universal import RavenUniversalAnalyzer, interactive_raven_universal
    UNIVERSAL_AVAILABLE = True
    print("‚úÖ Analizador Universal importado correctamente")
except ImportError as e:
    print(f"‚ö†Ô∏è Raven Universal no encontrado - funcionando en modo cl√°sico: {e}")
    UNIVERSAL_AVAILABLE = False
    # Crear funci√≥n dummy para evitar errores
    def interactive_raven_universal():
        print("‚ùå Analizador Universal no disponible")
        print("üí° Instala el m√≥dulo raven_universal.py para habilitarlo")

# *** IMPORTACIONES PARA APRENDIZAJE GRATUITO ***
try:
    from core.free_learning import RavenFreeLearningSystem, interactive_free_learning
    FREE_LEARNING_AVAILABLE = True
    print("‚úÖ Sistema de aprendizaje gratuito cargado")
except ImportError as e:
    print(f"‚ö†Ô∏è Aprendizaje gratuito no disponible: {e}")
    FREE_LEARNING_AVAILABLE = False
    def interactive_free_learning():
        print("‚ùå Sistema de aprendizaje gratuito no disponible")

# *** IMPORTACIONES PARA ENTRENAMIENTO AI ***
try:
    from core.training_mode import RavenTrainingMode, interactive_training_mode, TrainedRavenEnhancement
    TRAINING_MODE_AVAILABLE = True
    print("‚úÖ Modo de entrenamiento AI cargado")
except ImportError as e:
    print(f"‚ö†Ô∏è Modo de entrenamiento no disponible: {e}")
    TRAINING_MODE_AVAILABLE = False
    async def interactive_training_mode():
        print("‚ùå Modo de entrenamiento AI no disponible")
    class TrainedRavenEnhancement:
        def __init__(self): self.has_trained_knowledge = False
        def enhance_classification(self, analysis, features): return analysis

# *** NUEVAS IMPORTACIONES PARA INTEGRACI√ìN AI ***
try:
    from core.ai_integration import RavenAIIntegration, EnhancedRavenWithAI
    AI_INTEGRATION_AVAILABLE = True
    print("‚úÖ M√≥dulo de integraci√≥n AI cargado correctamente")
except ImportError as e:
    print(f"‚ö†Ô∏è Integraci√≥n AI no disponible: {e}")
    print("üí° Para habilitar AI: pip install openai anthropic")
    AI_INTEGRATION_AVAILABLE = False
    # Crear clases dummy para evitar errores
    class RavenAIIntegration:
        def __init__(self, *args, **kwargs):
            pass
    class EnhancedRavenWithAI:
        def __init__(self, *args, **kwargs):
            pass

# *** NUEVA FUNCI√ìN PARA VERIFICAR AI EN TIEMPO REAL ***
def check_ai_integration_runtime():
    """
    Verificaci√≥n mejorada en tiempo de ejecuci√≥n para detectar si las dependencias de IA est√°n disponibles.
    Esta funci√≥n reemplaza la verificaci√≥n est√°tica y es m√°s confiable.
    """
    ai_status = {
        'anthropic': False,
        'openai': False,
        'available': False
    }
    
    # Verificar Anthropic
    try:
        import anthropic
        # Test b√°sico de funcionamiento
        try:
            # Solo test de importaci√≥n sin crear cliente real
            ai_status['anthropic'] = True
            print("‚úÖ Anthropic SDK disponible")
        except Exception:
            ai_status['anthropic'] = True  # Si se puede importar, est√° disponible
            print("‚úÖ Anthropic SDK disponible")
    except ImportError:
        print("‚ùå Anthropic SDK no encontrado")
    except Exception as e:
        print(f"‚ö†Ô∏è Anthropic SDK con problema: {e}")
    
    # Verificar OpenAI
    try:
        import openai
        ai_status['openai'] = True
        print("‚úÖ OpenAI SDK disponible")
    except ImportError:
        print("‚ùå OpenAI SDK no encontrado")
    except Exception as e:
        print(f"‚ö†Ô∏è OpenAI SDK con problema: {e}")
    
    # Determinar disponibilidad general
    ai_status['available'] = ai_status['anthropic'] or ai_status['openai']
    
    return ai_status

def natural_sort_key(s):
    """Funci√≥n para ordenar num√©ricamente los nombres de archivo."""
    return [int(text) if text.isdigit() else text.lower() 
            for text in re.split('([0-9]+)', s)]

def save_enhanced_classification(image_path, cluster, description, features, analysis):
    """Guarda clasificaci√≥n mejorada con an√°lisis detallado en JSON"""
    json_path = os.path.splitext(image_path)[0] + '.json'
    
    # Convertir arrays numpy a listas para JSON serialization
    serializable_features = {}
    for key, value in features.items():
        if isinstance(value, np.ndarray):
            serializable_features[key] = value.tolist()
        elif isinstance(value, (np.integer, np.floating)):
            serializable_features[key] = float(value)
        else:
            serializable_features[key] = value
    
    classification_data = {
        'image_filename': os.path.basename(image_path),
        'cluster': int(cluster),
        'cluster_name': analysis.get('cluster_name', f'Cluster {cluster}'),
        'cluster_description': description,
        'confidence': float(analysis.get('confidence', 0.0)),
        'fractal_features': serializable_features,
        'feature_analysis': analysis.get('feature_matches', {}),
        'similar_clusters': analysis.get('similar_clusters', []),
        'recommendations': analysis.get('recommendations', []),
        'classification_method': {
            'kmeans_prediction': analysis.get('kmeans_prediction', cluster),
            'rule_prediction': analysis.get('rule_prediction', cluster),
            'final_method': analysis.get('final_method', 'hybrid')
        },
        'last_processed': datetime.now().strftime("%d-%m-%Y %H:%M:%S"), 
        'version': '2.1_complete_system'
    }
    
    # *** A√ëADIR MEJORAS DE ENTRENAMIENTO SI EST√ÅN DISPONIBLES ***
    if analysis.get('used_trained_knowledge'):
        classification_data['trained_enhancements'] = {
            'improvements': analysis.get('trained_improvements', []),
            'confidence_boost': analysis.get('confidence_boost_from_training', 0.0),
            'enhanced_by_training': True
        }
    
    try:
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(classification_data, f, indent=4, ensure_ascii=False)
        print(f"üìä An√°lisis guardado: {os.path.basename(json_path)}")
        if analysis.get('used_trained_knowledge'):
            print(f"üß† Incluye mejoras de entrenamiento AI")
    except Exception as e:
        print(f"‚ùå Error guardando JSON para {image_path}: {e}")

# *** NUEVA FUNCI√ìN PARA GUARDAR CON AI INSIGHTS ***
def save_enhanced_classification_with_ai(image_path, cluster, description, features, analysis, ai_insights=None):
    """Guarda clasificaci√≥n mejorada con an√°lisis detallado en JSON + insights AI"""
    json_path = os.path.splitext(image_path)[0] + '.json'
    
    # Convertir arrays numpy a listas para JSON serialization
    serializable_features = {}
    for key, value in features.items():
        if isinstance(value, np.ndarray):
            serializable_features[key] = value.tolist()
        elif isinstance(value, (np.integer, np.floating)):
            serializable_features[key] = float(value)
        else:
            serializable_features[key] = value
    
    classification_data = {
        'image_filename': os.path.basename(image_path),
        'cluster': int(cluster),
        'cluster_name': analysis.get('cluster_name', f'Cluster {cluster}'),
        'cluster_description': description,
        'confidence': float(analysis.get('confidence', 0.0)),
        'fractal_features': serializable_features,
        'feature_analysis': analysis.get('feature_matches', {}),
        'similar_clusters': analysis.get('similar_clusters', []),
        'recommendations': analysis.get('recommendations', []),
        'classification_method': {
            'kmeans_prediction': analysis.get('kmeans_prediction', cluster),
            'rule_prediction': analysis.get('rule_prediction', cluster),
            'final_method': analysis.get('final_method', 'hybrid')
        },
        'last_processed': datetime.now().strftime("%d-%m-%Y %H:%M:%S"), 
        'version': '2.1_ai_integrated'
    }
    
    # *** A√ëADIR INSIGHTS AI SI EST√ÅN DISPONIBLES ***
    if ai_insights:
        classification_data['ai_analysis'] = {
            'consensus_level': ai_insights.get('consensus', {}).get('agreement_level', 'none'),
            'ai_confidence': ai_insights.get('consensus', {}).get('confidence_avg', 0.0),
            'ai_recommendation': ai_insights.get('recommendation', ''),
            'models_consulted': [r['model'] for r in ai_insights.get('responses', [])],
            'confidence_boost': ai_insights.get('confidence_boost', 0.0)
        }
    
    try:
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(classification_data, f, indent=4, ensure_ascii=False)
        print(f"üìä An√°lisis guardado: {os.path.basename(json_path)}")
        if ai_insights:
            print(f"ü§ñ Incluye insights AI: {ai_insights.get('consensus', {}).get('agreement_level', 'none')} consensus")
    except Exception as e:
        print(f"‚ùå Error guardando JSON para {image_path}: {e}")

def extract_comprehensive_features(interpreter, hausdorff_extractor, contour_extractor, image_path):
    """
    Extrae caracter√≠sticas completas usando todos los extractores.
    """
    try:
        # Caracter√≠sticas b√°sicas
        basic_features = interpreter.extract_features(image_path)
        
        # Cargar imagen para extractores adicionales
        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        if image is None:
            raise ValueError(f"No se pudo cargar la imagen: {image_path}")
        
        # Caracter√≠sticas de dimensi√≥n de Hausdorff
        hausdorff_features = hausdorff_extractor.extract(image)
        
        # Caracter√≠sticas de contornos
        contour_features = contour_extractor.extract(image)
        
        # Combinar todas las caracter√≠sticas
        comprehensive_features = {
            # Caracter√≠sticas b√°sicas
            'histogram': basic_features.histogram,
            'hu_moments': basic_features.hu_moments,
            'edge_density': basic_features.edge_density,
            'total_pixels': basic_features.total_pixels,
            
            # Caracter√≠sticas de Hausdorff
            'hausdorff_dimension': hausdorff_features['hausdorff_dimension'],
            'local_dimensions': hausdorff_features['local_dimensions'],
            'dimension_variance': hausdorff_features['dimension_variance'],
            'dimension_complexity': hausdorff_features['dimension_complexity'],
            'fractal_type': hausdorff_features['fractal_type'],
            
            # Caracter√≠sticas de contornos
            'contour_count': contour_features['contour_count'],
            'total_perimeter': contour_features['total_perimeter'],
            'avg_area': contour_features['avg_area'],
            'contour_complexity': contour_features['contour_complexity'],
            'circularity_mean': contour_features['circularity_mean'],
            'circularity_std': contour_features['circularity_std'],
            'convexity_mean': contour_features['convexity_mean'],
            'aspect_ratio_mean': contour_features['aspect_ratio_mean'],
            'contour_hierarchy_depth': contour_features['contour_hierarchy_depth']
        }
        
        return comprehensive_features
        
    except Exception as e:
        print(f"‚ùå Error extrayendo caracter√≠sticas de {image_path}: {e}")
        return None

def create_feature_vector(features):
    """Crea vector de caracter√≠sticas para clustering."""
    try:
        vector_components = []
        
        # Caracter√≠sticas b√°sicas (histograma reducido)
        if 'histogram' in features and len(features['histogram']) > 0:
            hist_reduced = features['histogram'][:20]
            vector_components.extend(hist_reduced)
        else:
            vector_components.extend([0.0] * 20)
        
        # Momentos de Hu
        if 'hu_moments' in features and len(features['hu_moments']) > 0:
            vector_components.extend(features['hu_moments'])
        else:
            vector_components.extend([0.0] * 7)
        
        # Caracter√≠sticas de Hausdorff
        vector_components.extend([
            features.get('hausdorff_dimension', 0.0),
            features.get('dimension_complexity', 0.0),
            features.get('dimension_variance', 0.0)
        ])
        
        # Caracter√≠sticas de contornos m√°s importantes
        vector_components.extend([
            features.get('edge_density', 0.0),
            features.get('circularity_mean', 0.0),
            features.get('contour_complexity', 0.0),
            features.get('convexity_mean', 0.0),
            np.log1p(features.get('contour_count', 0))
        ])
        
        # Estad√≠sticas de dimensiones locales
        local_dims = features.get('local_dimensions', np.zeros(16))
        if len(local_dims) > 0:
            vector_components.extend([
                np.mean(local_dims),
                np.std(local_dims),
                np.max(local_dims),
                np.min(local_dims)
            ])
        else:
            vector_components.extend([0.0, 0.0, 0.0, 0.0])
        
        return np.array(vector_components, dtype=np.float32)
        
    except Exception as e:
        print(f"‚ùå Error creando vector de caracter√≠sticas: {e}")
        return np.zeros(39, dtype=np.float32)

class FixedEnhancedPatternClassifier:
    """Clasificador mejorado CORREGIDO que funciona correctamente."""
    
    def __init__(self, n_clusters=10):
        self.n_clusters = n_clusters
        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        self.knowledge_base = EnhancedKnowledgeBase()
        self.is_fitted = False
        
    def fit(self, feature_vectors, comprehensive_features_list):
        """Entrena el clasificador con vectores de caracter√≠sticas."""
        try:
            self.kmeans.fit(feature_vectors)
            self.is_fitted = True
            print(f"ü§ñ Clasificador entrenado con {len(feature_vectors)} muestras")
            
            # Mostrar estad√≠sticas de clusters
            self._print_cluster_statistics(feature_vectors, comprehensive_features_list)
            
        except Exception as e:
            print(f"‚ùå Error entrenando clasificador: {e}")
    
    def predict_enhanced(self, feature_vector, comprehensive_features):
        """
        *** VERSI√ìN CORREGIDA ***
        Predicci√≥n mejorada que funciona correctamente.
        """
        if not self.is_fitted:
            return 0, 0.0, {'error': 'Classifier not fitted'}
        
        try:
            # Predicci√≥n por K-means
            kmeans_cluster = self.kmeans.predict([feature_vector])[0]
            
            # *** CORRECI√ìN PRINCIPAL ***
            # An√°lisis basado en reglas usando base de conocimiento
            rule_cluster, rule_confidence, all_scores = self.knowledge_base.classify_by_features(
                comprehensive_features
            )
            
            # Combinar ambos enfoques
            final_cluster, confidence, final_method = self._combine_predictions_fixed(
                kmeans_cluster, rule_cluster, rule_confidence, all_scores
            )
            
            # *** GENERAR AN√ÅLISIS DETALLADO ***
            analysis = self.knowledge_base.get_cluster_analysis(final_cluster, comprehensive_features)
            analysis['confidence'] = confidence
            analysis['kmeans_prediction'] = int(kmeans_cluster)
            analysis['rule_prediction'] = int(rule_cluster)
            analysis['final_method'] = final_method
            analysis['all_cluster_scores'] = {str(k): float(v) for k, v in all_scores.items()}
            analysis['similar_clusters'] = self.knowledge_base.suggest_similar_clusters(
                final_cluster, comprehensive_features
            )
            
            return final_cluster, confidence, analysis
            
        except Exception as e:
            print(f"‚ùå Error en predicci√≥n mejorada: {e}")
            return 0, 0.0, {'error': str(e)}
    
    def _combine_predictions_fixed(self, kmeans_cluster, rule_cluster, rule_confidence, all_scores):
        """
        *** VERSI√ìN CORREGIDA ***
        Combina predicciones de manera m√°s inteligente.
        """
        # Si la confianza de las reglas es muy alta, usar predicci√≥n por reglas
        if rule_confidence > 0.8:
            return rule_cluster, rule_confidence, 'rule_based_high_confidence'
        
        # Si las predicciones coinciden, aumentar confianza
        if kmeans_cluster == rule_cluster:
            combined_confidence = min(0.95, rule_confidence + 0.3)
            return rule_cluster, combined_confidence, 'hybrid_agreement'
        
        # Si no coinciden, evaluar cu√°l es m√°s confiable basado en scores
        kmeans_confidence = 0.6  # Confianza base para K-means
        
        # Si la diferencia de scores es significativa, usar reglas
        sorted_scores = sorted(all_scores.values(), reverse=True)
        score_gap = sorted_scores[0] - sorted_scores[1] if len(sorted_scores) > 1 else 0
        
        if score_gap > 0.2:  # Gap significativo en scores de reglas
            return rule_cluster, rule_confidence + score_gap * 0.5, 'rule_based_significant_gap'
        
        # Si la confianza de reglas es mayor, usar reglas
        if rule_confidence > kmeans_confidence:
            return rule_cluster, rule_confidence, 'rule_based_higher_confidence'
        else:
            # Usar K-means pero con confianza reducida por el desacuerdo
            return kmeans_cluster, kmeans_confidence * 0.7, 'kmeans_with_disagreement'
    
    def _print_cluster_statistics(self, feature_vectors, comprehensive_features_list):
        """Imprime estad√≠sticas de los clusters formados."""
        try:
            labels = self.kmeans.labels_
            
            print("\nüìä Estad√≠sticas de Clustering:")
            for cluster_id in range(self.n_clusters):
                cluster_mask = labels == cluster_id
                cluster_count = np.sum(cluster_mask)
                percentage = (cluster_count / len(labels)) * 100
                
                cluster_name = self.knowledge_base.get_cluster_name(cluster_id)
                print(f"  Cluster {cluster_id} ({cluster_name}): {cluster_count} im√°genes ({percentage:.1f}%)")
                
                if cluster_count > 0:
                    # Estad√≠sticas de dimensi√≥n de Hausdorff para este cluster
                    hausdorff_dims = [
                        features.get('hausdorff_dimension', 0.0) 
                        for i, features in enumerate(comprehensive_features_list) 
                        if cluster_mask[i]
                    ]
                    if hausdorff_dims:
                        avg_hausdorff = np.mean(hausdorff_dims)
                        print(f"    - Dimensi√≥n Hausdorff promedio: {avg_hausdorff:.3f}")
                        
        except Exception as e:
            print(f"‚ùå Error calculando estad√≠sticas: {e}")

def run_fractal_analysis():
    """Funci√≥n que ejecuta el an√°lisis fractal original."""
    print("üöÄ Iniciando Raven v2.1 SISTEMA COMPLETO...")

    # Inicializar componentes
    interpreter = FractalInterpreter()
    hausdorff_extractor = HausdorffDimensionExtractor()
    contour_extractor = ContourAnalysisExtractor()
    
    # A√±adir extractores al int√©rprete
    interpreter.add_extractor('hausdorff', hausdorff_extractor)
    interpreter.add_extractor('contours', contour_extractor)
    
    # *** USAR CLASIFICADOR CORREGIDO ***
    enhanced_classifier = FixedEnhancedPatternClassifier(n_clusters=10)
    scaler = StandardScaler()

    # *** BUSCAR CARPETA DEL D√çA CON FOLDER ANALYZER CORREGIDO ***
    print("üîç Buscando carpeta del d√≠a con FolderAnalyzer corregido...")
    
    # Usar None para autodetecci√≥n de rutas
    today_folder = FolderAnalyzer.get_todays_folder()

    if not today_folder:
        print("‚ö†Ô∏è No se encontr√≥ carpeta del d√≠a. Intentando crear...")
        today_folder = FolderAnalyzer.create_todays_folder()
        
        if not today_folder:
            print("‚ùå No se pudo crear carpeta del d√≠a.")
            print("üí° Verifica la estructura de carpetas o crea manualmente:")
            print("    data/today/DDMMYYYY")
            return
        else:
            print(f"‚úÖ Carpeta creada: {today_folder}")

    print(f"‚úÖ Carpeta de hoy encontrada: {today_folder}")

    # Verificar que la carpeta tenga contenido
    try:
        all_files = os.listdir(today_folder)
        print(f"üìÅ Total de archivos en carpeta: {len(all_files)}")
        
        if len(all_files) == 0:
            print("‚ö†Ô∏è La carpeta est√° vac√≠a")
            print("üí° Copia im√°genes fractales a la carpeta antes de ejecutar")
            return
        
        # Mostrar primeros archivos para debug
        print("üìÇ Primeros archivos encontrados:")
        for i, file in enumerate(all_files[:5]):
            print(f"   {i+1}. {file}")
        if len(all_files) > 5:
            print(f"   ... y {len(all_files) - 5} archivos m√°s")
        
    except Exception as e:
        print(f"‚ùå Error accediendo a la carpeta: {e}")
        return

    # *** FILTRO CORREGIDO ***
    # Obtener y ordenar archivos (VERSI√ìN CORREGIDA)
    image_files = sorted(
        [f for f in os.listdir(today_folder) if f.lower().endswith((".png", ".jpg", ".jpeg"))],
        key=natural_sort_key
    )
    
    if not image_files:
        print("‚ùå No se encontraron im√°genes en la carpeta")
        print("üí° Formatos soportados: .png, .jpg, .jpeg")
        return

    print(f"üñºÔ∏è Encontradas {len(image_files)} im√°genes para procesar")

    # Procesar im√°genes y extraer caracter√≠sticas completas
    feature_vectors = []
    comprehensive_features_list = []
    processed_files = []

    print("\nüîç Extrayendo caracter√≠sticas avanzadas...")
    for i, fname in enumerate(image_files):
        path = os.path.join(today_folder, fname)
        print(f"  Procesando {i+1}/{len(image_files)}: {fname}")
        
        try:
            # Extraer caracter√≠sticas completas
            comprehensive_features = extract_comprehensive_features(
                interpreter, hausdorff_extractor, contour_extractor, path
            )
            
            if comprehensive_features is not None:
                # Crear vector para clustering
                feature_vector = create_feature_vector(comprehensive_features)
                
                feature_vectors.append(feature_vector)
                comprehensive_features_list.append(comprehensive_features)
                processed_files.append(fname)
                
                # Mostrar dimensi√≥n de Hausdorff y tipo fractal
                hausdorff_dim = comprehensive_features.get('hausdorff_dimension', 0.0)
                fractal_type = comprehensive_features.get('fractal_type', 'unknown')
                print(f"     Dimensi√≥n Hausdorff: {hausdorff_dim:.3f} | Tipo: {fractal_type}")
                
        except Exception as e:
            print(f"     ‚ùå Error procesando {fname}: {e}")

    if not feature_vectors:
        print("‚ùå No se pudieron procesar im√°genes v√°lidas")
        return

    print(f"\n‚úÖ Procesadas {len(feature_vectors)} im√°genes exitosamente")

    # Escalado y entrenamiento del clasificador
    print("\nü§ñ Entrenando clasificador h√≠brido corregido...")
    feature_vectors_np = np.array(feature_vectors)
    scaled_vectors = scaler.fit_transform(feature_vectors_np)
    enhanced_classifier.fit(scaled_vectors, comprehensive_features_list)

    # Clasificaci√≥n y an√°lisis detallado
    print("\nüìä Resultados de clasificaci√≥n h√≠brida corregida:")
    print("=" * 80)
    
    classification_summary = {i: 0 for i in range(10)}
    
    for i, (feature_vector, comprehensive_features, fname) in enumerate(
        zip(feature_vectors_np, comprehensive_features_list, processed_files)
    ):
        img_path = os.path.join(today_folder, fname)
        scaled_vector = scaler.transform([feature_vector])[0]
        
        # *** PREDICCI√ìN MEJORADA CORREGIDA CON ENTRENAMIENTO AI ***
        cluster, confidence, analysis = enhanced_classifier.predict_enhanced(
            scaled_vector, comprehensive_features
        )
        
        # Aplicar mejoras de conocimiento entrenado si est√° disponible
        if TRAINING_MODE_AVAILABLE:
            enhancement = TrainedRavenEnhancement()
            if enhancement.has_trained_knowledge:
                analysis = enhancement.enhance_classification(analysis, comprehensive_features)
                confidence = analysis.get('confidence', confidence)
        
        classification_summary[cluster] += 1
        
        # Descripci√≥n del cluster
        cluster_name = analysis.get('cluster_name', f'Cluster {cluster}')
        description = enhanced_classifier.knowledge_base.describe_cluster(cluster)
        
        # Mostrar resultado detallado
        print(f"\nüñºÔ∏è {fname}")
        print(f"     Cluster: {cluster} - {cluster_name}")
        print(f"    üìù {description}")
        print(f"    üéØ Confianza: {confidence:.3f} | M√©todo: {analysis.get('final_method', 'hybrid')}")
        
        # Mostrar si hay mejoras de entrenamiento
        if analysis.get('used_trained_knowledge'):
            boost = analysis.get('confidence_boost_from_training', 0.0)
            print(f"    üß† Mejorado con entrenamiento AI (+{boost:.3f} confianza)")
        
        # Mostrar caracter√≠sticas clave
        hausdorff_dim = comprehensive_features.get('hausdorff_dimension', 0.0)
        complexity = comprehensive_features.get('dimension_complexity', 0.0)
        contour_count = comprehensive_features.get('contour_count', 0)
        circularity = comprehensive_features.get('circularity_mean', 0.0)
        
        print(f"    üìê Dim. Hausdorff: {hausdorff_dim:.3f} | Complejidad: {complexity:.3f}")
        print(f"    üîç Contornos: {contour_count} | Circularidad: {circularity:.3f}")
        
        # Mostrar predicciones de ambos m√©todos
        kmeans_pred = analysis.get('kmeans_prediction', cluster)
        rule_pred = analysis.get('rule_prediction', cluster)
        print(f"    ü§ñ K-means: {kmeans_pred} | Reglas: {rule_pred}")
        
        # Mostrar clusters similares
        similar_clusters = analysis.get('similar_clusters', [])
        if similar_clusters and len(similar_clusters) > 0:
            similar_info = similar_clusters[0]
            similar_name = enhanced_classifier.knowledge_base.get_cluster_name(similar_info[0])
            print(f"    üîó Similar a: Cluster {similar_info[0]} ({similar_name}) - Score: {similar_info[1]:.3f}")
        
        # Mostrar recomendaciones
        recommendations = analysis.get('recommendations', [])
        if recommendations:
            print(f"    üí° Recomendaci√≥n: {recommendations[0]}")
        
        # Guardar an√°lisis completo
        save_enhanced_classification(img_path, cluster, description, 
                                   comprehensive_features, analysis)

    # Resumen final
    print("\n" + "=" * 80)
    print("üìã RESUMEN DE CLASIFICACI√ìN COMPLETA")
    print("=" * 80)
    
    knowledge_base = enhanced_classifier.knowledge_base
    
    for cluster_id in range(10):
        count = classification_summary[cluster_id]
        if count > 0:
            percentage = (count / len(processed_files)) * 100
            cluster_name = knowledge_base.get_cluster_name(cluster_id)
            print(f"  Cluster {cluster_id} ({cluster_name}): {count} im√°genes ({percentage:.1f}%)")
    
    # Estad√≠sticas globales de dimensi√≥n de Hausdorff
    all_hausdorff_dims = [f.get('hausdorff_dimension', 0.0) for f in comprehensive_features_list]
    if all_hausdorff_dims:
        avg_hausdorff = np.mean(all_hausdorff_dims)
        std_hausdorff = np.std(all_hausdorff_dims)
        min_hausdorff = np.min(all_hausdorff_dims)
        max_hausdorff = np.max(all_hausdorff_dims)
        
        print(f"\nüìä ESTAD√çSTICAS DE DIMENSI√ìN DE HAUSDORFF:")
        print(f"   Promedio: {avg_hausdorff:.3f} ¬± {std_hausdorff:.3f}")
        print(f"   Rango: [{min_hausdorff:.3f}, {max_hausdorff:.3f}]")
    
    # Distribuci√≥n de tipos fractales
    fractal_types = [f.get('fractal_type', 'unknown') for f in comprehensive_features_list]
    type_counts = {}
    for ftype in fractal_types:
        type_counts[ftype] = type_counts.get(ftype, 0) + 1
    
    print(f"\nüî¨ DISTRIBUCI√ìN DE TIPOS FRACTALES:")
    for ftype, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / len(fractal_types)) * 100
        print(f"   {ftype}: {count} im√°genes ({percentage:.1f}%)")

    # *** MOVER CARPETA PROCESADA CON FOLDER ANALYZER ***
    print(f"\nüì¶ Moviendo carpeta procesada...")
    
    # Obtener ruta base del proyecto
    project_root = FolderAnalyzer._get_project_root()
    processed_dir = os.path.join(project_root, "data", "processed")
    
    if not os.path.exists(processed_dir):
        os.makedirs(processed_dir)
    
    # Obtener nombre base de la carpeta
    folder_name = datetime.now().strftime("%d%m%Y")
    
    # Buscar un nombre √∫nico con numeraci√≥n autom√°tica
    counter = 1
    while True:
        new_folder_name = f"{folder_name}_analyzed_{counter}"
        destination = os.path.join(processed_dir, new_folder_name)
        
        # Si no existe esta carpeta, usar este nombre
        if not os.path.exists(destination):
            break
        
        # Si existe, incrementar contador y probar siguiente
        counter += 1
        
        # Prevenir bucle infinito (m√°ximo 999 intentos)
        if counter > 999:
            print(f"‚ö†Ô∏è Demasiadas carpetas con la misma fecha. Usando timestamp.")
            timestamp = datetime.now().strftime("%d%m%Y_%H%M%S")
            new_folder_name = f"{folder_name}_analyzed_{timestamp}"
            destination = os.path.join(processed_dir, new_folder_name)
            break
    
    try:
        shutil.move(today_folder, destination)
        print(f"‚úÖ Carpeta movida a procesados: {destination}")
        print(f"üìÅ Nombre asignado: {folder_name} ‚Üí {new_folder_name}")
        if counter > 1:
            print(f"üìä An√°lisis #{counter} del d√≠a")
    except Exception as e:
        print(f"‚ùå Error moviendo carpeta: {e}")

    print(f"\nüéâ ¬°Clasificaci√≥n h√≠brida completada!")

# *** NUEVA FUNCI√ìN PARA AN√ÅLISIS FRACTAL CON AI ***
async def run_fractal_analysis_with_ai(openai_key=None, anthropic_key=None):
    """Funci√≥n que ejecuta el an√°lisis fractal con integraci√≥n AI."""
    print("üöÄ Iniciando Raven v2.1 con INTEGRACI√ìN AI...")
    
    # Verificar disponibilidad de AI
    if not AI_INTEGRATION_AVAILABLE:
        print("‚ö†Ô∏è Integraci√≥n AI no disponible, ejecutando an√°lisis cl√°sico")
        run_fractal_analysis()  # Llamar funci√≥n original
        return
    
    if not (openai_key or anthropic_key):
        print("‚ö†Ô∏è No se proporcionaron claves API, ejecutando an√°lisis cl√°sico")
        run_fractal_analysis()  # Llamar funci√≥n original
        return
    
    # Crear instancia mejorada de Raven con AI
    try:
        enhanced_raven = EnhancedRavenWithAI(openai_key, anthropic_key)
        print("ü§ñ Raven con capacidades AI iniciado exitosamente")
    except Exception as e:
        print(f"‚ùå Error iniciando integraci√≥n AI: {e}")
        print("üîÑ Continuando con an√°lisis cl√°sico...")
        run_fractal_analysis()
        return
    
    print("üîç Buscando carpeta del d√≠a...")
    today_folder = FolderAnalyzer.get_todays_folder()
    
    if not today_folder:
        print("‚ùå No se encontr√≥ carpeta del d√≠a")
        return
    
    # Obtener im√°genes
    image_files = sorted(
        [f for f in os.listdir(today_folder) if f.lower().endswith((".png", ".jpg", ".jpeg"))],
        key=natural_sort_key
    )
    
    if not image_files:
        print("‚ùå No se encontraron im√°genes")
        return
    
    print(f"üñºÔ∏è Encontradas {len(image_files)} im√°genes para an√°lisis AI-mejorado")
    
    # Procesar cada imagen con an√°lisis AI (ejemplo para primeras 3 im√°genes)
    for i, fname in enumerate(image_files[:3]):  # Procesar solo primeras 3 para demo
        img_path = os.path.join(today_folder, fname)
        print(f"\nüîç Analizando con AI: {fname} ({i+1}/{min(3, len(image_files))})")
        
        try:
            # An√°lisis mejorado con consenso AI
            enhanced_analysis = await enhanced_raven.analyze_with_ai_consensus(img_path)
            
            # Mostrar resultados
            raven_cluster = enhanced_analysis['raven_analysis']['cluster_analysis']['cluster_name']
            ai_agreement = enhanced_analysis['ai_consensus']['consensus']['agreement_level']
            final_confidence = enhanced_analysis['confidence_boost']
            
            print(f"üìä An√°lisis Raven: {raven_cluster}")
            print(f"ü§ñ Consenso AI: {ai_agreement}")
            print(f"‚ú® Confianza final: {final_confidence:.3f}")
            
            # Guardar con insights AI
            save_enhanced_classification_with_ai(
                img_path,
                enhanced_analysis['raven_analysis']['cluster_analysis'].get('cluster_id', 0),
                enhanced_analysis['raven_analysis']['cluster_analysis'].get('cluster_name', ''),
                enhanced_analysis['raven_analysis']['features'],
                enhanced_analysis['raven_analysis']['cluster_analysis'],
                enhanced_analysis['ai_consensus']
            )
            
        except Exception as e:
            print(f"‚ùå Error en an√°lisis AI para {fname}: {e}")
            continue
    
    print("\nüéâ ¬°An√°lisis fractal con AI completado!")

# *** FUNCI√ìN SHOW_MAIN_MENU CORREGIDA CON DETECCI√ìN AI EN TIEMPO REAL ***
def show_main_menu():
    """Muestra el men√∫ principal integrado con todas las opciones."""
    print("\n" + "=" * 70)
    print("ü¶Ö RAVEN - SISTEMA DE AN√ÅLISIS FRACTAL INTEGRADO v2.1 COMPLETO")
    print("=" * 70)
    print("\nüéØ Selecciona el modo de an√°lisis:")
    print("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
    
    print("\n1. üìä An√°lisis Fractal de Carpeta (Modo Cl√°sico)")
    print("   ‚îî‚îÄ Procesamiento tradicional de Raven")
    print("   ‚îî‚îÄ Clasificaci√≥n en 10 clusters especializados")
    print("   ‚îî‚îÄ An√°lisis de dimensi√≥n de Hausdorff avanzado")
    print("   ‚îî‚îÄ 100% GRATUITO")
    
    if FREE_LEARNING_AVAILABLE:
        print("\n2. üß† Aprendizaje Gratuito")
        print("   ‚îî‚îÄ Raven aprende de tus an√°lisis hist√≥ricos")
        print("   ‚îî‚îÄ Correcciones manuales para mejorar precisi√≥n")
        print("   ‚îî‚îÄ Auto-mejora basada en tus datos")
        print("   ‚îî‚îÄ 100% GRATUITO - Sin APIs externas")
    else:
        print("\n2. üß† Aprendizaje Gratuito (No disponible)")
        print("   ‚îî‚îÄ Falta core/free_learning.py")
    
    if TRAINING_MODE_AVAILABLE:
        print("\n3. üéì Entrenar Raven con AI (Una vez)")
        print("   ‚îî‚îÄ GPT-4 + Claude entrenan a Raven")
        print("   ‚îî‚îÄ Inversi√≥n √∫nica: $5-15")
        print("   ‚îî‚îÄ Despu√©s funciona GRATIS para siempre")
        print("   ‚îî‚îÄ Mejora permanente del sistema")
    else:
        print("\n3. üéì Entrenar Raven con AI (No disponible)")
        print("   ‚îî‚îÄ Falta core/training_mode.py o dependencias")
    
    # *** VERIFICACI√ìN AI MEJORADA EN TIEMPO REAL ***
    print("\nüîç Verificando dependencias AI...")
    ai_runtime_status = check_ai_integration_runtime()
    
    if ai_runtime_status['available']:
        print("\n4. ü§ñ An√°lisis Fractal con AI (GPT-4 + Claude)")
        print("   ‚îî‚îÄ An√°lisis Raven + consenso de modelos AI")
        print("   ‚îî‚îÄ Clasificaci√≥n verificada por GPT-4 y Claude")
        print("   ‚îî‚îÄ Confianza mejorada basada en consenso")
        print("   ‚îî‚îÄ ‚ö†Ô∏è Requiere claves API v√°lidas")
        
        # Mostrar qu√© SDKs est√°n disponibles
        available_models = []
        if ai_runtime_status['anthropic']:
            available_models.append("Claude")
        if ai_runtime_status['openai']:
            available_models.append("GPT-4")
        print(f"   ‚îî‚îÄ SDKs disponibles: {', '.join(available_models)}")
    else:
        print("\n4. ‚ùå An√°lisis Fractal con AI (No disponible)")
        print("   ‚îî‚îÄ Dependencias faltantes detectadas:")
        if not ai_runtime_status['anthropic']:
            print("       ‚Ä¢ pip install anthropic")
        if not ai_runtime_status['openai']:
            print("       ‚Ä¢ pip install openai")
    
    if UNIVERSAL_AVAILABLE:
        print("\n5. üåü An√°lisis Universal")
        print("   ‚îî‚îÄ Analiza CUALQUIER tipo de datos")
        print("   ‚îî‚îÄ Texto, n√∫meros, URLs, archivos, JSON, CSV")
    else:
        print("\n5. üåü An√°lisis Universal (No disponible)")
        print("   ‚îî‚îÄ Falta core/raven_universal.py")
    
    print("\n6. üìã Informaci√≥n del Sistema")
    print("7. üö™ Salir")
    
    # *** MOSTRAR ESTADOS DE DISPONIBILIDAD ACTUALIZADOS ***
    print(f"\nüìä Estado de funcionalidades (verificado en tiempo real):")
    print(f"   üîÑ An√°lisis Cl√°sico: ‚úÖ Siempre disponible")
    print(f"   üß† Aprendizaje Gratuito: {'‚úÖ' if FREE_LEARNING_AVAILABLE else '‚ùå'}")
    print(f"   üéì Entrenamiento AI: {'‚úÖ' if TRAINING_MODE_AVAILABLE else '‚ùå'}")
    print(f"   ü§ñ Integraci√≥n AI: {'‚úÖ' if ai_runtime_status['available'] else '‚ùå'}")
    print(f"   üåü An√°lisis Universal: {'‚úÖ' if UNIVERSAL_AVAILABLE else '‚ùå'}")
    
    # Guardar estado AI para uso en main()
    global AI_RUNTIME_AVAILABLE
    AI_RUNTIME_AVAILABLE = ai_runtime_status['available']

def show_system_info():
    """Muestra informaci√≥n completa del sistema."""
    print("\n" + "=" * 60)
    print("üìã INFORMACI√ìN DEL SISTEMA RAVEN v2.1 COMPLETO")
    print("=" * 60)
    
    print("\nüîß COMPONENTES PRINCIPALES:")
    components = [
        ("Int√©rprete Fractal", True),
        ("Clasificador de Patrones", True), 
        ("Analizador de Carpetas", True),
        ("Extractor Hausdorff", True),
        ("Base de Conocimiento (10 clusters)", True),
        ("An√°lisis Universal", UNIVERSAL_AVAILABLE),
        ("Aprendizaje Gratuito", FREE_LEARNING_AVAILABLE),
        ("Entrenamiento AI", TRAINING_MODE_AVAILABLE),
        ("Integraci√≥n AI", AI_INTEGRATION_AVAILABLE)
    ]
    
    for name, available in components:
        status = "‚úÖ Disponible" if available else "‚ùå No disponible"
        print(f"   {name}: {status}")
    
    print(f"\nüéØ CAPACIDADES ACTIVAS:")
    print(f"   ‚Ä¢ An√°lisis fractal especializado: ‚úÖ")
    print(f"   ‚Ä¢ Clasificaci√≥n en 10 clusters: ‚úÖ")
    print(f"   ‚Ä¢ Dimensi√≥n de Hausdorff: ‚úÖ")
    print(f"   ‚Ä¢ An√°lisis de contornos: ‚úÖ")
    print(f"   ‚Ä¢ Base de conocimiento expandida: ‚úÖ")
    print(f"   ‚Ä¢ Aprendizaje de datos hist√≥ricos: {'‚úÖ' if FREE_LEARNING_AVAILABLE else '‚ùå'}")
    print(f"   ‚Ä¢ Entrenamiento con AI: {'‚úÖ' if TRAINING_MODE_AVAILABLE else '‚ùå'}")
    print(f"   ‚Ä¢ Integraci√≥n AI en tiempo real: {'‚úÖ' if AI_INTEGRATION_AVAILABLE else '‚ùå'}")
    print(f"   ‚Ä¢ An√°lisis universal: {'‚úÖ' if UNIVERSAL_AVAILABLE else '‚ùå'}")
    
    # Verificar si hay conocimiento entrenado
    if TRAINING_MODE_AVAILABLE:
        enhancement = TrainedRavenEnhancement()
        if enhancement.has_trained_knowledge:
            print(f"   üß† Conocimiento AI entrenado: ‚úÖ ACTIVO")
            print(f"       ‚îî‚îÄ Raven funcionar√° con mejoras AI autom√°ticamente")
        else:
            print(f"   üß† Conocimiento AI entrenado: ‚ùå Sin entrenar")
            print(f"       ‚îî‚îÄ Usa opci√≥n 3 para entrenar Raven con AI")
    
    print(f"\nüìä TIPOS DE FRACTALES RECONOCIDOS:")
    try:
        kb = EnhancedKnowledgeBase()
        for i in range(10):
            cluster_name = kb.get_cluster_name(i)
            print(f"   {i}. {cluster_name}")
    except Exception as e:
        print(f"   ‚ùå Error accediendo a la base de conocimiento: {e}")
    
    print(f"\nüóÇÔ∏è  ESTRUCTURA DE DATOS:")
    print(f"   üìÅ data/ - Carpetas de im√°genes por fecha")
    print(f"   üìÅ data/processed/ - An√°lisis completados")
    print(f"   üìÅ data/trained_knowledge/ - Conocimiento AI entrenado")
    print(f"   üìÑ *.json - Metadatos de cada imagen")

    # Mostrar estado actual de carpetas
    print(f"\nüìã ESTADO ACTUAL:")
    try:
        # Verificar carpeta del d√≠a
        today_folder = FolderAnalyzer.get_todays_folder()
        if today_folder:
            print(f"   ‚úÖ Carpeta del d√≠a: {os.path.basename(today_folder)}")
            stats = FolderAnalyzer.get_folder_stats(today_folder)
            if "error" not in stats:
                print(f"   üìä Archivos disponibles: {stats['files']}")
                if stats['file_types']:
                    for ext, count in stats['file_types'].items():
                        print(f"      {ext}: {count}")
        else:
            print(f"   ‚ùå No hay carpeta del d√≠a actual")
        
        # Fechas disponibles
        dates = FolderAnalyzer.list_available_dates()
        if dates:
            print(f"   üìÖ Fechas disponibles: {len(dates)}")
            for date in dates[:3]:
                formatted = f"{date[0:2]}/{date[2:4]}/{date[4:8]}"
                print(f"      ‚Ä¢ {date} ({formatted})")
            if len(dates) > 3:
                print(f"      ... y {len(dates) - 3} m√°s")
        else:
            print(f"   üìÖ No hay fechas disponibles")
            
    except Exception as e:
        print(f"   ‚ö†Ô∏è Error verificando estado: {e}")

# *** FUNCI√ìN MAIN CORREGIDA ***
def main():
    """Funci√≥n principal con men√∫ integrado completo."""
    
    print("üöÄ INICIANDO RAVEN v2.1 - SISTEMA COMPLETO")
    print("=" * 60)
    
    # Verificar FolderAnalyzer al inicio
    try:
        test_result = FolderAnalyzer._get_project_root()
        print(f"‚úÖ FolderAnalyzer funcionando - Proyecto en: {test_result}")
    except Exception as e:
        print(f"‚ùå Error en FolderAnalyzer: {e}")
        print("üí° Verifica que core/folder_analyzer.py est√© actualizado")
        input("Presiona Enter para continuar de todos modos...")
    
    # *** VERIFICACI√ìN INICIAL DE DEPENDENCIAS AI ***
    print("\nüîç Verificaci√≥n inicial de dependencias AI...")
    ai_initial_check = check_ai_integration_runtime()
    if ai_initial_check['available']:
        print("‚úÖ Dependencias AI detectadas correctamente")
    else:
        print("‚ö†Ô∏è Algunas dependencias AI no est√°n disponibles")
    
    while True:
        show_main_menu()
        
        try:
            choice = input("\nüëâ Selecciona una opci√≥n (1-7): ").strip()
            
            if choice == '1':
                print("\nüîÑ Iniciando an√°lisis fractal cl√°sico...")
                run_fractal_analysis()
                input("\n‚ú® Presiona Enter para continuar...")
                
            elif choice == '2':
                if FREE_LEARNING_AVAILABLE:
                    print("\nüß† Iniciando sistema de aprendizaje gratuito...")
                    try:
                        interactive_free_learning()
                    except Exception as e:
                        print(f"‚ùå Error en aprendizaje gratuito: {e}")
                    input("\n‚ú® Presiona Enter para continuar...")
                else:
                    print("\n‚ùå Aprendizaje gratuito no disponible")
                    print("üí° Crea el archivo core/free_learning.py")
                    input("\n‚ú® Presiona Enter para continuar...")
                    
            elif choice == '3':
                if TRAINING_MODE_AVAILABLE:
                    print("\nüéì Iniciando modo de entrenamiento AI...")
                    try:
                        # Verificar si ya est√° entrenado
                        enhancement = TrainedRavenEnhancement()
                        if enhancement.has_trained_knowledge:
                            print("üß† Raven ya tiene conocimiento entrenado!")
                            print("‚úÖ El sistema ya funciona con mejoras AI")
                            
                            retrain = input("¬øQuieres volver a entrenar? (s/n): ").strip().lower()
                            if retrain not in ['s', 's√≠', 'si', 'y', 'yes']:
                                continue
                        
                        # Ejecutar entrenamiento
                        asyncio.run(interactive_training_mode())
                    except Exception as e:
                        print(f"‚ùå Error en entrenamiento: {e}")
                    input("\n‚ú® Presiona Enter para continuar...")
                else:
                    print("\n‚ùå Modo de entrenamiento no disponible")
                    print("üí° Crea core/training_mode.py e instala: pip install openai anthropic")
                    input("\n‚ú® Presiona Enter para continuar...")
                    
            elif choice == '4':
                # *** VERIFICACI√ìN AI CORREGIDA ***
                print("\nüîç Verificando disponibilidad AI en tiempo real...")
                ai_check = check_ai_integration_runtime()
                
                if ai_check['available']:
                    print("‚úÖ AI disponible - Continuando con configuraci√≥n...")
                    print("\nü§ñ CONFIGURACI√ìN DE INTEGRACI√ìN AI")
                    print("=" * 40)
                    print("Necesitas claves API para:")
                    if ai_check['openai']:
                        print("‚Ä¢ ‚úÖ OpenAI GPT-4: https://platform.openai.com/api-keys")
                    else:
                        print("‚Ä¢ ‚ùå OpenAI no disponible")
                    if ai_check['anthropic']:
                        print("‚Ä¢ ‚úÖ Anthropic Claude: https://console.anthropic.com/")
                    else:
                        print("‚Ä¢ ‚ùå Anthropic no disponible")
                    
                    print("\n‚ö†Ô∏è Las claves se usan solo para esta sesi√≥n (no se almacenan)")
                    
                    # Solicitar claves API solo para los SDKs disponibles
                    openai_key = ""
                    anthropic_key = ""
                    
                    if ai_check['openai']:
                        openai_key = input("\nüîë Clave OpenAI (Enter para omitir): ").strip()
                    
                    if ai_check['anthropic']:
                        anthropic_key = input("üîë Clave Anthropic (Enter para omitir): ").strip()
                    
                    if openai_key or anthropic_key:
                        print(f"\nüöÄ Iniciando an√°lisis AI...")
                        print(f"   GPT-4: {'‚úÖ' if openai_key and ai_check['openai'] else '‚ùå'}")
                        print(f"   Claude: {'‚úÖ' if anthropic_key and ai_check['anthropic'] else '‚ùå'}")
                        
                        # Ejecutar an√°lisis AI de forma as√≠ncrona
                        try:
                            if AI_INTEGRATION_AVAILABLE:  # Verificar que el m√≥dulo est√° importado
                                asyncio.run(run_fractal_analysis_with_ai(openai_key, anthropic_key))
                            else:
                                print("‚ö†Ô∏è M√≥dulo AI no cargado, ejecutando an√°lisis cl√°sico")
                                run_fractal_analysis()
                        except Exception as e:
                            print(f"‚ùå Error en an√°lisis AI: {e}")
                            print("üîÑ Ejecutando an√°lisis cl√°sico...")
                            run_fractal_analysis()
                    else:
                        print("‚ùå No se proporcionaron claves, ejecutando an√°lisis cl√°sico")
                        run_fractal_analysis()
                    
                    input("\n‚ú® Presiona Enter para continuar...")
                else:
                    print("\n‚ùå Integraci√≥n AI no disponible")
                    print("üîß Dependencias faltantes:")
                    if not ai_check['anthropic']:
                        print("   pip install anthropic")
                    if not ai_check['openai']:
                        print("   pip install openai")
                    print("\nüí° Instala las dependencias y reinicia RAVEN")
                    input("\n‚ú® Presiona Enter para continuar...")
                    
            elif choice == '5':
                if UNIVERSAL_AVAILABLE:
                    print("\nüåü Iniciando Raven Universal...")
                    try:
                        interactive_raven_universal()
                    except Exception as e:
                        print(f"‚ùå Error en an√°lisis universal: {e}")
                    input("\n‚ú® Presiona Enter para continuar...")
                else:
                    print("\n‚ùå An√°lisis Universal no disponible")
                    print("üí° Instala core/raven_universal.py para habilitarlo")
                    input("\n‚ú® Presiona Enter para continuar...")
                    
            elif choice == '6':
                show_system_info()
                input("\n‚ú® Presiona Enter para continuar...")
                
            elif choice == '7':
                print("\nüëã ¬°Gracias por usar Raven v2.1 Completo!")
                print("ü¶Ö Sistema de an√°lisis fractal con m√°ximas capacidades")
                print("üîß Aprendizaje gratuito + Entrenamiento AI disponibles")
                break
                
            else:
                print("‚ùå Opci√≥n no v√°lida. Selecciona 1-7.")
                
        except KeyboardInterrupt:
            print("\n\nüëã Salida interrumpida por el usuario")
            break
        except Exception as e:
            print(f"\n‚ùå Error inesperado: {e}")
            import traceback
            traceback.print_exc()
            input("‚ú® Presiona Enter para continuar...")

# *** INICIALIZAR VARIABLE GLOBAL ***
AI_RUNTIME_AVAILABLE = False

if __name__ == "__main__":
    main()