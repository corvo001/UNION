#!/usr/bin/env python3
"""
RAVEN TRAINING MODE
Sistema de entrenamiento temporal que usa AI para mejorar Raven y luego funciona GRATIS

Concepto:
1. FASE ENTRENAMIENTO (pagada): GPT-4/Claude analizan 100-200 im√°genes
2. EXTRACCI√ìN: Raven aprende de sus respuestas
3. FASE PRODUCCI√ìN (gratis): Raven funciona solo con conocimiento mejorado
"""

import json
import os
import numpy as np
from datetime import datetime
from typing import Dict, List, Any
import asyncio

# Solo importar AI si est√° disponible
try:
    from core.ai_integration import RavenAIIntegration
    AI_AVAILABLE = True
except ImportError:
    AI_AVAILABLE = False

class RavenTrainingMode:
    """
    Modo de entrenamiento que usa AI temporalmente para mejorar Raven permanentemente.
    
    INVERSI√ìN INICIAL: $5-20 para entrenar
    RESULTADO: Raven mejorado que funciona GRATIS para siempre
    """
    
    def __init__(self, training_budget=10.0):  # $10 USD presupuesto
        self.training_budget = training_budget  # Presupuesto m√°ximo
        self.spent_so_far = 0.0
        self.training_data = {
            'ai_insights': [],
            'pattern_discoveries': [],
            'improved_definitions': {},
            'new_rules': [],
            'confidence_adjustments': {}
        }
        self.cost_per_analysis = 0.02  # ~$0.02 por imagen
        self.max_training_samples = int(training_budget / self.cost_per_analysis)
        
        print(f"üí∞ Modo Entrenamiento iniciado - Presupuesto: ${training_budget}")
        print(f"üìä M√°ximo {self.max_training_samples} im√°genes de entrenamiento")
        
        if AI_AVAILABLE:
            self.ai_integration = None  # Se inicializa con claves
        
    def estimate_training_cost(self, num_images: int) -> Dict[str, Any]:
        """
        Estima el costo de entrenar con X im√°genes
        
        Args:
            num_images: N√∫mero de im√°genes para entrenar
            
        Returns:
            Estimaci√≥n detallada de costos
        """
        cost_breakdown = {
            'images': num_images,
            'cost_per_image': self.cost_per_analysis,
            'total_estimated_cost': num_images * self.cost_per_analysis,
            'tokens_per_image': 800,  # Promedio prompt + respuesta
            'total_tokens': num_images * 800,
            'within_budget': num_images * self.cost_per_analysis <= self.training_budget
        }
        
        # Beneficios estimados
        expected_improvements = {
            'confidence_boost': f"+{min(25, num_images * 0.2):.1f}%",
            'accuracy_improvement': f"+{min(15, num_images * 0.1):.1f}%",
            'new_patterns_discovered': min(10, num_images // 20),
            'cluster_refinements': min(8, num_images // 25)
        }
        
        cost_breakdown['expected_benefits'] = expected_improvements
        
        return cost_breakdown
    
    def interactive_training_setup(self):
        """
        Configuraci√≥n interactiva del entrenamiento
        """
        print("\nüéì CONFIGURACI√ìN DEL ENTRENAMIENTO RAVEN")
        print("=" * 50)
        print("El objetivo es gastar una vez para mejorar Raven permanentemente")
        
        # Mostrar opciones de entrenamiento
        training_options = [
            {"name": "Entrenamiento B√°sico", "images": 50, "cost": 1.0, "improvement": "Mejora b√°sica"},
            {"name": "Entrenamiento Est√°ndar", "images": 150, "cost": 3.0, "improvement": "Mejora significativa"},
            {"name": "Entrenamiento Avanzado", "images": 300, "cost": 6.0, "improvement": "Mejora considerable"},
            {"name": "Entrenamiento Completo", "images": 500, "cost": 10.0, "improvement": "Mejora m√°xima"}
        ]
        
        print("\nüéØ Opciones de entrenamiento disponibles:")
        print("-" * 60)
        
        for i, option in enumerate(training_options, 1):
            cost_estimate = self.estimate_training_cost(option["images"])
            print(f"\n{i}. {option['name']}")
            print(f"   üñºÔ∏è Im√°genes: {option['images']}")
            print(f"   üí∞ Costo estimado: ${option['cost']:.2f}")
            print(f"   üìà Resultado: {option['improvement']}")
            print(f"   ‚ú® Beneficios esperados:")
            for benefit, value in cost_estimate['expected_benefits'].items():
                print(f"      ‚Ä¢ {benefit.replace('_', ' ').title()}: {value}")
        
        print(f"\n5. ‚öôÔ∏è Configuraci√≥n personalizada")
        print(f"6. üö™ Cancelar")
        
        choice = input(f"\nüëâ Selecciona opci√≥n (1-6): ").strip()
        
        if choice in ['1', '2', '3', '4']:
            selected_option = training_options[int(choice) - 1]
            return self._confirm_training_plan(selected_option)
        elif choice == '5':
            return self._custom_training_setup()
        elif choice == '6':
            print("‚ùå Entrenamiento cancelado")
            return None
        else:
            print("‚ùå Opci√≥n inv√°lida")
            return None
    
    def _confirm_training_plan(self, option: Dict) -> Dict:
        """Confirma el plan de entrenamiento seleccionado"""
        print(f"\nüìã PLAN DE ENTRENAMIENTO SELECCIONADO")
        print("=" * 40)
        print(f"üì¶ Plan: {option['name']}")
        print(f"üñºÔ∏è Im√°genes a procesar: {option['images']}")
        print(f"üí∞ Costo estimado: ${option['cost']:.2f}")
        print(f"üìà Mejora esperada: {option['improvement']}")
        
        print(f"\n‚ö†Ô∏è IMPORTANTE:")
        print(f"‚Ä¢ Este es un gasto √∫nico para entrenar Raven")
        print(f"‚Ä¢ Despu√©s del entrenamiento, Raven funciona GRATIS")
        print(f"‚Ä¢ El conocimiento adquirido se guarda permanentemente")
        print(f"‚Ä¢ No necesitar√°s pagar m√°s APIs despu√©s del entrenamiento")
        
        confirm = input(f"\n¬øProceder con el entrenamiento? (s/n): ").strip().lower()
        
        if confirm in ['s', 's√≠', 'si', 'y', 'yes']:
            return {
                'images_to_train': option['images'],
                'estimated_cost': option['cost'],
                'plan_name': option['name']
            }
        else:
            print("‚ùå Entrenamiento cancelado")
            return None
    
    def _custom_training_setup(self) -> Dict:
        """Configuraci√≥n personalizada de entrenamiento"""
        print(f"\n‚öôÔ∏è CONFIGURACI√ìN PERSONALIZADA")
        print(f"üí∞ Presupuesto disponible: ${self.training_budget:.2f}")
        print(f"üí° Costo estimado por imagen: ${self.cost_per_analysis:.3f}")
        
        try:
            num_images = int(input(f"üñºÔ∏è ¬øCu√°ntas im√°genes quieres usar para entrenar? (1-{self.max_training_samples}): "))
            
            if 1 <= num_images <= self.max_training_samples:
                cost_estimate = self.estimate_training_cost(num_images)
                
                print(f"\nüìä ESTIMACI√ìN PERSONALIZADA:")
                print(f"   üñºÔ∏è Im√°genes: {num_images}")
                print(f"   üí∞ Costo total: ${cost_estimate['total_estimated_cost']:.2f}")
                print(f"   üìà Mejoras esperadas:")
                for benefit, value in cost_estimate['expected_benefits'].items():
                    print(f"      ‚Ä¢ {benefit.replace('_', ' ').title()}: {value}")
                
                confirm = input(f"\n¬øProceder con esta configuraci√≥n? (s/n): ").strip().lower()
                
                if confirm in ['s', 's√≠', 'si', 'y', 'yes']:
                    return {
                        'images_to_train': num_images,
                        'estimated_cost': cost_estimate['total_estimated_cost'],
                        'plan_name': 'Personalizado'
                    }
            else:
                print(f"‚ùå N√∫mero de im√°genes fuera de rango")
        except ValueError:
            print(f"‚ùå Entrada inv√°lida")
        
        return None
    
    async def execute_training(self, training_plan: Dict, openai_key: str = None, anthropic_key: str = None):
        """
        Ejecuta el entrenamiento usando AI temporalmente
        
        Args:
            training_plan: Plan de entrenamiento confirmado
            openai_key: Clave API de OpenAI
            anthropic_key: Clave API de Anthropic
        """
        if not AI_AVAILABLE:
            print("‚ùå M√≥dulo AI no disponible para entrenamiento")
            return False
        
        if not (openai_key or anthropic_key):
            print("‚ùå Se necesita al menos una clave API para entrenar")
            return False
        
        print(f"\nüöÄ INICIANDO ENTRENAMIENTO: {training_plan['plan_name']}")
        print("=" * 50)
        
        # Inicializar integraci√≥n AI
        self.ai_integration = RavenAIIntegration(openai_key, anthropic_key)
        
        # Obtener im√°genes de entrenamiento
        training_images = self._select_training_images(training_plan['images_to_train'])
        
        if not training_images:
            print("‚ùå No se encontraron im√°genes suficientes para entrenar")
            return False
        
        print(f"üñºÔ∏è Entrenando con {len(training_images)} im√°genes")
        print(f"üí∞ Costo estimado: ${training_plan['estimated_cost']:.2f}")
        
        # Entrenar con cada imagen
        successful_trainings = 0
        total_cost = 0.0
        
        for i, image_path in enumerate(training_images, 1):
            if total_cost >= self.training_budget:
                print(f"‚ö†Ô∏è Presupuesto agotado en imagen {i}")
                break
            
            print(f"\nüì∏ Entrenando {i}/{len(training_images)}: {os.path.basename(image_path)}")
            
            try:
                # Realizar an√°lisis AI
                training_result = await self._train_with_single_image(image_path)
                
                if training_result['success']:
                    successful_trainings += 1
                    total_cost += training_result['estimated_cost']
                    
                    # Guardar insights del entrenamiento
                    self.training_data['ai_insights'].append(training_result)
                    
                    print(f"   ‚úÖ Entrenamiento exitoso")
                    print(f"   üí∞ Costo acumulado: ${total_cost:.3f}")
                else:
                    print(f"   ‚ùå Error in entrenamiento: {training_result.get('error', 'Unknown')}")
                
            except Exception as e:
                print(f"   ‚ùå Error inesperado: {e}")
                continue
        
        print(f"\nüéâ ENTRENAMIENTO COMPLETADO")
        print(f"‚úÖ Im√°genes procesadas exitosamente: {successful_trainings}")
        print(f"üí∞ Costo total real: ${total_cost:.2f}")
        
        # Generar conocimiento mejorado
        improved_knowledge = self._extract_knowledge_from_training()
        
        # Guardar conocimiento permanentemente
        self._save_improved_knowledge(improved_knowledge)
        
        print(f"\nüß† CONOCIMIENTO EXTRA√çDO Y GUARDADO")
        print(f"üéØ Raven ahora funcionar√° GRATIS con este conocimiento mejorado")
        
        return True
    
    def _select_training_images(self, num_images: int) -> List[str]:
        """Selecciona im√°genes representativas para entrenamiento"""
        # Buscar im√°genes en carpetas procesadas y actuales
        all_images = []
        search_paths = ["data/processed", "data/today"]
        
        for search_path in search_paths:
            if os.path.exists(search_path):
                for root, dirs, files in os.walk(search_path):
                    for file in files:
                        if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                            full_path = os.path.join(root, file)
                            all_images.append(full_path)
        
        if len(all_images) < num_images:
            print(f"‚ö†Ô∏è Solo se encontraron {len(all_images)} im√°genes (necesarias: {num_images})")
            return all_images
        
        # Seleccionar im√°genes diversas (distribuidas uniformemente)
        selected_indices = np.linspace(0, len(all_images) - 1, num_images, dtype=int)
        selected_images = [all_images[i] for i in selected_indices]
        
        return selected_images
    
    async def _train_with_single_image(self, image_path: str) -> Dict[str, Any]:
        """Entrena con una sola imagen usando AI"""
        try:
            # Preparar prompt de entrenamiento especializado
            training_prompt = f"""
            ENTRENAMIENTO DE RAVEN - An√°lisis de Fractal

            Est√°s ayudando a entrenar un sistema de an√°lisis fractal llamado Raven.
            Analiza esta imagen fractal y proporciona:

            1. Tipo de fractal principal (Mandelbrot, Julia, IFS, etc.)
            2. Dimensi√≥n de Hausdorff estimada (1.0-3.0)
            3. Caracter√≠sticas visuales clave
            4. Nivel de complejidad (1-10)
            5. Patrones geom√©tricos dominantes
            6. Recomendaciones para clasificaci√≥n autom√°tica

            S√© espec√≠fico y t√©cnico - este an√°lisis mejorar√° las reglas de clasificaci√≥n.
            """
            
            context = {
                'training_mode': True,
                'image_path': image_path,
                'purpose': 'knowledge_extraction'
            }
            
            # Consultar AI
            ai_result = await self.ai_integration.multi_model_consensus(training_prompt, context)
            
            if 'error' not in ai_result:
                return {
                    'success': True,
                    'image_path': image_path,
                    'ai_responses': ai_result['responses'],
                    'consensus': ai_result['consensus'],
                    'recommendation': ai_result['recommendation'],
                    'estimated_cost': self.cost_per_analysis,
                    'training_insights': self._extract_training_insights(ai_result)
                }
            else:
                return {
                    'success': False,
                    'error': ai_result['error'],
                    'estimated_cost': 0.0
                }
        
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'estimated_cost': 0.0
            }
    
    def _extract_training_insights(self, ai_result: Dict) -> Dict[str, Any]:
        """Extrae insights espec√≠ficos para mejorar Raven"""
        insights = {
            'fractal_characteristics': [],
            'classification_rules': [],
            'confidence_indicators': [],
            'pattern_recognition': []
        }
        
        # Analizar respuestas AI para extraer conocimiento estructurado
        for response in ai_result.get('responses', []):
            response_text = response.get('response', '').lower()
            
            # Buscar menciones de tipos fractales
            fractal_types = ['mandelbrot', 'julia', 'ifs', 'cantor', 'sierpinski', 'lorenz', 'henon']
            for ftype in fractal_types:
                if ftype in response_text:
                    insights['fractal_characteristics'].append({
                        'type': ftype,
                        'model': response.get('model'),
                        'confidence': response.get('confidence')
                    })
            
            # Buscar valores num√©ricos (dimensiones, etc.)
            import re
            dimension_matches = re.findall(r'dimensi[√≥o]n[^0-9]*([0-9]+\.?[0-9]*)', response_text)
            for dim_str in dimension_matches:
                try:
                    dimension = float(dim_str)
                    if 1.0 <= dimension <= 3.0:
                        insights['classification_rules'].append({
                            'type': 'hausdorff_dimension',
                            'value': dimension,
                            'source_model': response.get('model')
                        })
                except ValueError:
                    continue
        
        return insights
    
    def _extract_knowledge_from_training(self) -> Dict[str, Any]:
        """Extrae conocimiento permanente del entrenamiento"""
        print("\nüß† Extrayendo conocimiento permanente del entrenamiento...")
        
        improved_knowledge = {
            'cluster_refinements': {},
            'new_classification_rules': [],
            'confidence_boosters': {},
            'pattern_recognition_improvements': [],
            'training_metadata': {
                'total_samples': len(self.training_data['ai_insights']),
                'training_date': datetime.now().isoformat(),
                'models_used': []
            }
        }
        
        # Analizar todos los insights de entrenamiento
        all_insights = []
        models_used = set()
        
        for training_result in self.training_data['ai_insights']:
            if training_result.get('success'):
                all_insights.append(training_result['training_insights'])
                
                # Registrar modelos usados
                for response in training_result.get('ai_responses', []):
                    models_used.add(response.get('model', 'unknown'))
        
        improved_knowledge['training_metadata']['models_used'] = list(models_used)
        
        # Consolidar caracter√≠sticas fractales
        fractal_consolidation = {}
        for insight in all_insights:
            for char in insight.get('fractal_characteristics', []):
                ftype = char['type']
                if ftype not in fractal_consolidation:
                    fractal_consolidation[ftype] = {'mentions': 0, 'total_confidence': 0.0}
                
                fractal_consolidation[ftype]['mentions'] += 1
                fractal_consolidation[ftype]['total_confidence'] += char.get('confidence', 0.5)
        
        # Crear reglas de clasificaci√≥n mejoradas
        for ftype, data in fractal_consolidation.items():
            avg_confidence = data['total_confidence'] / data['mentions']
            if data['mentions'] >= 3 and avg_confidence > 0.6:  # Suficiente evidencia
                improved_knowledge['new_classification_rules'].append({
                    'rule_type': 'fractal_type_detection',
                    'fractal_type': ftype,
                    'confidence_threshold': avg_confidence,
                    'evidence_strength': data['mentions'],
                    'description': f'Regla mejorada para detectar fractales tipo {ftype}'
                })
        
        # Consolidar reglas de dimensi√≥n
        dimension_rules = []
        for insight in all_insights:
            for rule in insight.get('classification_rules', []):
                if rule['type'] == 'hausdorff_dimension':
                    dimension_rules.append(rule['value'])
        
        if dimension_rules:
            # Crear rangos mejorados basados en dimensiones observadas
            dimension_ranges = {
                'low_dimension': [d for d in dimension_rules if d < 1.5],
                'medium_dimension': [d for d in dimension_rules if 1.5 <= d < 2.0],
                'high_dimension': [d for d in dimension_rules if d >= 2.0]
            }
            
            improved_knowledge['confidence_boosters']['dimension_classification'] = dimension_ranges
        
        print(f"‚úÖ Conocimiento extra√≠do:")
        print(f"   ‚Ä¢ {len(improved_knowledge['new_classification_rules'])} nuevas reglas")
        print(f"   ‚Ä¢ {len(fractal_consolidation)} tipos fractales identificados")
        print(f"   ‚Ä¢ {len(dimension_rules)} referencias de dimensi√≥n")
        
        return improved_knowledge
    
    def _save_improved_knowledge(self, knowledge: Dict[str, Any]):
        """Guarda el conocimiento mejorado permanentemente"""
        # Crear directorio si no existe
        knowledge_dir = "data/trained_knowledge"
        os.makedirs(knowledge_dir, exist_ok=True)
        
        # Guardar conocimiento completo
        timestamp = datetime.now().strftime("%d%m%Y_%H%M%S")
        knowledge_file = os.path.join(knowledge_dir, f"raven_trained_knowledge_{timestamp}.json")
        
        with open(knowledge_file, 'w', encoding='utf-8') as f:
            json.dump(knowledge, f, indent=2, ensure_ascii=False)
        
        # Tambi√©n guardar versi√≥n "activa" que Raven cargar√° autom√°ticamente
        active_knowledge_file = os.path.join(knowledge_dir, "active_trained_knowledge.json")
        with open(active_knowledge_file, 'w', encoding='utf-8') as f:
            json.dump(knowledge, f, indent=2, ensure_ascii=False)
        
        print(f"üíæ Conocimiento guardado en: {knowledge_file}")
        print(f"üéØ Conocimiento activo: {active_knowledge_file}")
        print(f"üÜì Raven ahora usar√° este conocimiento GRATIS en futuros an√°lisis")


# INTEGRACI√ìN CON EL SISTEMA PRINCIPAL DE RAVEN

class TrainedRavenEnhancement:
    """
    Mejoras para Raven basadas en conocimiento entrenado (GRATUITO despu√©s del entrenamiento)
    """
    
    def __init__(self):
        self.trained_knowledge = self._load_trained_knowledge()
        self.has_trained_knowledge = self.trained_knowledge is not None
        
        if self.has_trained_knowledge:
            print("üß† Conocimiento entrenado cargado - Raven funcionar√° con mejoras AI")
        else:
            print("üîÑ Sin conocimiento entrenado - Raven funcionar√° en modo est√°ndar")
    
    def _load_trained_knowledge(self) -> Dict[str, Any]:
        """Carga conocimiento entrenado si existe"""
        knowledge_file = "data/trained_knowledge/active_trained_knowledge.json"
        
        if os.path.exists(knowledge_file):
            try:
                with open(knowledge_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"‚ö†Ô∏è Error cargando conocimiento entrenado: {e}")
        
        return None
    
    def enhance_classification(self, original_analysis: Dict, features: Dict) -> Dict:
        """
        Mejora la clasificaci√≥n usando conocimiento entrenado (GRATIS)
        
        Args:
            original_analysis: An√°lisis original de Raven
            features: Caracter√≠sticas extra√≠das
            
        Returns:
            An√°lisis mejorado con conocimiento entrenado
        """
        if not self.has_trained_knowledge:
            return original_analysis
        
        enhanced_analysis = original_analysis.copy()
        confidence_boost = 0.0
        improvements = []
        
        # Aplicar reglas de clasificaci√≥n aprendidas
        new_rules = self.trained_knowledge.get('new_classification_rules', [])
        for rule in new_rules:
            if rule['rule_type'] == 'fractal_type_detection':
                # Verificar si las caracter√≠sticas coinciden con el tipo fractal aprendido
                fractal_type = features.get('fractal_type', '').lower()
                if rule['fractal_type'] in fractal_type:
                    confidence_boost += 0.1
                    improvements.append(f"Coincidencia con patr√≥n entrenado: {rule['fractal_type']}")
        
        # Aplicar boosters de confianza
        confidence_boosters = self.trained_knowledge.get('confidence_boosters', {})
        if 'dimension_classification' in confidence_boosters:
            hausdorff_dim = features.get('hausdorff_dimension', 0.0)
            
            # Verificar en qu√© rango cae la dimensi√≥n
            for range_name, dimensions in confidence_boosters['dimension_classification'].items():
                if dimensions and min(dimensions) <= hausdorff_dim <= max(dimensions):
                    confidence_boost += 0.05
                    improvements.append(f"Dimensi√≥n en rango entrenado: {range_name}")
        
        # Aplicar mejoras
        if confidence_boost > 0:
            original_confidence = enhanced_analysis.get('confidence', 0.0)
            enhanced_analysis['confidence'] = min(1.0, original_confidence + confidence_boost)
            enhanced_analysis['trained_improvements'] = improvements
            enhanced_analysis['confidence_boost_from_training'] = confidence_boost
            enhanced_analysis['used_trained_knowledge'] = True
        
        return enhanced_analysis


async def interactive_training_mode():
    """Funci√≥n interactiva para el modo de entrenamiento"""
    if not AI_AVAILABLE:
        print("‚ùå M√≥dulo AI no disponible")
        print("üí° Instala las dependencias: pip install openai anthropic")
        print("üí° Crea el archivo core/ai_integration.py")
        return
    
    trainer = RavenTrainingMode(training_budget=15.0)
    
    # Configurar entrenamiento
    training_plan = trainer.interactive_training_setup()
    
    if not training_plan:
        return
    
    # Solicitar claves API
    print(f"\nüîë CONFIGURACI√ìN DE API KEYS")
    print("Necesarias solo para el entrenamiento:")
    
    openai_key = input("üîë Clave OpenAI (Enter para omitir): ").strip()
    anthropic_key = input("üîë Clave Anthropic (Enter para omitir): ").strip()
    
    if not (openai_key or anthropic_key):
        print("‚ùå Se necesita al menos una clave API para entrenar")
        return
    
    # Ejecutar entrenamiento
    success = await trainer.execute_training(training_plan, openai_key, anthropic_key)
    
    if success:
        print(f"\nüéâ ¬°ENTRENAMIENTO COMPLETADO EXITOSAMENTE!")
        print(f"üÜì Raven ahora funciona GRATIS con conocimiento mejorado")
        print(f"üí° Usa la opci√≥n '1' del men√∫ principal para an√°lisis mejorados")
    else:
        print(f"\n‚ùå Entrenamiento fall√≥")

if __name__ == "__main__":
    # Ejemplo de uso
    print("Modo entrenamiento de Raven")
    asyncio.run(interactive_training_mode())